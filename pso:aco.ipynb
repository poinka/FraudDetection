{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Classifiers\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Additional encoders\n",
    "import category_encoders as ce\n",
    "\n",
    "# Stats\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import (\n",
    "    PCA,\n",
    "    KernelPCA, \n",
    "    FastICA,\n",
    "    TruncatedSVD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First and foremost, merge two dbs into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv(\"data/train_transaction.csv\")\n",
    "train_identity = pd.read_csv(\"data/train_identity.csv\")\n",
    "\n",
    "# Merge both dataframes on 'TransactionID'\n",
    "train = pd.merge(train_transaction, train_identity, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "print(f\"Rows in merged training set: {train.shape[0]}\")\n",
    "print(f\"Columns in merged training set: {train.shape[1]}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform an initial exploratory data analysis (EDA) by checking missing value percentages and examining the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the target variable distribution and missing values for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value percentages per column\n",
    "missing_percent = (train.isnull().sum() / len(train)) * 100\n",
    "missing_percent = missing_percent.sort_values(ascending=False)\n",
    "print(\"Missing percentages per column:\")\n",
    "print(missing_percent[missing_percent > 0])\n",
    "\n",
    "# Distribution of the target variable 'isFraud'\n",
    "train['isFraud'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Distribution of Fraudulent vs Non-Fraudulent Transactions\")\n",
    "plt.xlabel(\"isFraud\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cols = [col for col in train.columns if train[col].isna().sum() > 0.9 * len(train)]\n",
    "null_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = train.copy(deep=True)\n",
    "for col in null_cols:\n",
    "    missing_df[\"m_flag_\"+col] = np.where(missing_df[col].isnull(), 1, 0)\n",
    "    correlation = missing_df[[\"m_flag_\"+col, 'isFraud']].corr()\n",
    "    print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = train.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_features:\n",
    "    print(col, len(set(train[col])), set(train[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find truly categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Screen resolution values are true numerical values, while all other features are categorical in nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Screen_Width', 'Screen_Height']] = train['id_33'].str.split('x', expand=True).astype(float)\n",
    "train = train.drop(columns=['id_33'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Identify candidate categorical features based on unique value counts\n",
    "candidate_categorical = {}\n",
    "# Set a threshold for maximum unique values\n",
    "unique_threshold = 20\n",
    "\n",
    "# Iterate over numeric columns to check unique value counts\n",
    "for col in train.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    unique_vals = train[col].nunique()\n",
    "    if (unique_vals < unique_threshold) and (col != \"isFraud\"):\n",
    "        candidate_categorical[col] = unique_vals\n",
    "\n",
    "# Print candidate categorical features\n",
    "print(\"Candidate categorical features (numeric columns with few unique values):\")\n",
    "for col, count in candidate_categorical.items():\n",
    "    print(f\"{col}: {count} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not using standard imputation:\n",
    "1. Placed zero values as indicator for missing values where feature values no zero values anywhere else\n",
    "2. Added 'missing' instead of null for categorical values to keep all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical columns\n",
    "num_cols = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = list(set(cat_cols).union(set(candidate_categorical.keys())))\n",
    "num_cols = [col for col in num_cols if col not in cat_cols and col not in (\"TransactionID\", \"isFraud\")]\n",
    "\n",
    "# Imputation for numeric columns using zeros as indicator\n",
    "num_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "train[num_cols] = num_imputer.fit_transform(train[num_cols])\n",
    "\n",
    "# Imputation for categorical columns using a constant value\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "train[cat_cols] = cat_imputer.fit_transform(train[cat_cols])\n",
    "\n",
    "# Confirm that no missing values remain (or check overall missing count)\n",
    "print(\"Total missing values after imputation:\", train.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=[\"isFraud\", \"TransactionID\"])\n",
    "y = train['isFraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting train and test asap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using novel thing: WOE encoder to compensate for an enormous dimentionality for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_high = ce.WOEEncoder(cols=cat_cols)\n",
    "X_train_encoded_cat = encoder_high.fit_transform(X_train[cat_cols], y_train)\n",
    "X_test_encoded_cat = encoder_high.transform(X_test[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data to remove any disrepancies in SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled_num = scaler.fit_transform(X_train[num_cols])\n",
    "X_test_scaled_num = scaler.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(np.hstack([X_train_encoded_cat.values, X_train_scaled_num]), columns=cat_cols + num_cols)\n",
    "X_test_scaled = pd.DataFrame(np.hstack([X_test_encoded_cat.values, X_test_scaled_num]), columns=cat_cols + num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoded training set shape:\", X_train_scaled.shape)\n",
    "print(\"Encoded test set shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.to_csv(\"data/preprocessed_train.csv\", index=False)\n",
    "X_test_scaled.to_csv(\"data/preprocessed_test.csv\", index=False)\n",
    "\n",
    "y_train.to_csv(\"data/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"data/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.read_csv(\"data/preprocessed_train.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/preprocessed_test.csv\")\n",
    "y_train = pd.read_csv(\"data/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accounting for severe class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train to numpy array (since it's read as DataFrame)\n",
    "y_train_array = y_train['isFraud'].values\n",
    "\n",
    "# Get unique classes and compute weights\n",
    "unique_classes = np.unique(y_train_array)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=unique_classes,\n",
    "    y=y_train_array\n",
    ")\n",
    "\n",
    "# Create dictionary of class weights\n",
    "class_weights_dict = dict(zip(unique_classes, class_weights))\n",
    "class_weights_dict = {int(k): float(v) for k, v in class_weights_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent Component Analysis (ICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components=100, random_state=42)\n",
    "X_train_scaled = ica.fit_transform(X_train_scaled)\n",
    "X_test_scaled = ica.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_train_scaled = svd.fit_transform(X_train_scaled)\n",
    "X_test_scaled = svd.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_flat = y_train.values.ravel()\n",
    "y_test_flat = y_test.values.ravel()\n",
    "\n",
    "model_cat = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    class_weights=class_weights_dict,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "model_cat.fit(X_train_scaled, y_train_flat)\n",
    "y_pred_cat = model_cat.predict(X_test_scaled)\n",
    "print(\"CatBoost Training Accuracy:\", accuracy_score(y_test_flat, y_pred_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With all features:\n",
    "0:\tlearn: 0.6515508\ttotal: 127ms\tremaining: 1m 3s \n",
    "\n",
    "100:\tlearn: 0.3743155\ttotal: 13.5s\tremaining: 53.4s\n",
    "\n",
    "200:\tlearn: 0.3282363\ttotal: 26.9s\tremaining: 40s\n",
    "\n",
    "300:\tlearn: 0.2981647\ttotal: 40s\tremaining: 26.5s\n",
    "\n",
    "400:\tlearn: 0.2745897\ttotal: 53.3s\tremaining: 13.2s\n",
    "\n",
    "499:\tlearn: 0.2570046\ttotal: 1m 6s\tremaining: 0us\n",
    "\n",
    "CatBoost Training Accuracy: **0.9161614793240085**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With PCA:\n",
    "0:\tlearn: 0.6585685\ttotal: 31.1ms\tremaining: 15.5s\n",
    "\n",
    "100:\tlearn: 0.4369775\ttotal: 2.81s\tremaining: 11.1s\n",
    "\n",
    "200:\tlearn: 0.3974325\ttotal: 5.63s\tremaining: 8.38s\n",
    "\n",
    "300:\tlearn: 0.3670191\ttotal: 8.58s\tremaining: 5.67s\n",
    "\n",
    "400:\tlearn: 0.3430025\ttotal: 11.6s\tremaining: 2.86s\n",
    "\n",
    "499:\tlearn: 0.3227418\ttotal: 14.7s\tremaining: 0us\n",
    "\n",
    "CatBoost Training Accuracy: **0.8818962305686321**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With ISA:\n",
    "0:\tlearn: 0.6616616\ttotal: 32.2ms\tremaining: 16.1s\n",
    "\n",
    "100:\tlearn: 0.4328070\ttotal: 3.35s\tremaining: 13.2s\n",
    "\n",
    "200:\tlearn: 0.3877602\ttotal: 6.7s\tremaining: 9.97s\n",
    "\n",
    "300:\tlearn: 0.3543754\ttotal: 10.1s\tremaining: 6.69s\n",
    "\n",
    "400:\tlearn: 0.3284518\ttotal: 13.5s\tremaining: 3.33s\n",
    "\n",
    "499:\tlearn: 0.3067547\ttotal: 16.8s\tremaining: 0us\n",
    "\n",
    "CatBoost Training Accuracy: **0.8889490974362448**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With SVD:\n",
    "0:\tlearn: 0.6515508\ttotal: 133ms\tremaining: 1m 6s\n",
    "\n",
    "100:\tlearn: 0.3743155\ttotal: 13.2s\tremaining: 52s\n",
    "\n",
    "200:\tlearn: 0.3282363\ttotal: 26.8s\tremaining: 39.9s\n",
    "\n",
    "300:\tlearn: 0.2981647\ttotal: 40.3s\tremaining: 26.6s\n",
    "\n",
    "400:\tlearn: 0.2745897\ttotal: 54.2s\tremaining: 13.4s\n",
    "\n",
    "499:\tlearn: 0.2570046\ttotal: 1m 7s\tremaining: 0us\n",
    "\n",
    "CatBoost Training Accuracy: **0.9161614793240085**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA for input features subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.read_csv(\"data/preprocessed_train.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/preprocessed_test.csv\")\n",
    "y_train = pd.read_csv(\"data/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train to numpy array (since it's read as DataFrame)\n",
    "y_train_array = y_train['isFraud'].values\n",
    "\n",
    "# Get unique classes and compute weights\n",
    "unique_classes = np.unique(y_train_array)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=unique_classes,\n",
    "    y=y_train_array\n",
    ")\n",
    "\n",
    "# Create dictionary of class weights\n",
    "class_weights_dict = dict(zip(unique_classes, class_weights))\n",
    "class_weights_dict = {int(k): float(v) for k, v in class_weights_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_genetic_algorithm(X_data, y_data, population_size=30, n_generations=20, subset_size=100):\n",
    "    n_features = X_data.shape[1]\n",
    "    \n",
    "    # Initialize population - each individual is a sorted list of feature indices\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        subset = random.sample(range(n_features), subset_size)\n",
    "        subset.sort()\n",
    "        population.append(subset)\n",
    "    \n",
    "    def fitness(individual):\n",
    "        X_subset = X_data.iloc[:, individual]\n",
    "        \n",
    "        # Manual train/test split instead of cross-validation\n",
    "        X_train_subset, X_val_subset, y_train_subset, y_val_subset = train_test_split(\n",
    "            X_subset, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            model = CatBoostClassifier(\n",
    "                iterations=100,  # Reduce from 500 to speed up GA\n",
    "                learning_rate=0.1,\n",
    "                depth=6,\n",
    "                class_weights=class_weights_dict,\n",
    "                random_seed=42,\n",
    "                verbose=0        # Turn off verbosity completely            \n",
    "            )\n",
    "            \n",
    "            # Use a simple fit instead of cross_val_score\n",
    "            model.fit(X_train_subset, y_train_subset, \n",
    "                     eval_set=(X_val_subset, y_val_subset),\n",
    "                     early_stopping_rounds=20,\n",
    "                     verbose=False)\n",
    "            \n",
    "            # Get validation accuracy\n",
    "            accuracy = accuracy_score(y_val_subset, model.predict(X_val_subset))\n",
    "            print(f\"Feature subset evaluated: accuracy = {accuracy:.4f}\")\n",
    "            return accuracy\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fitness evaluation: {e}\")\n",
    "            return 0.0  # Return worst fitness on error\n",
    "    \n",
    "    # Creates a child by merging features from both parents and selecting a random subset\n",
    "    def crossover(p1, p2, subset_size):\n",
    "        combined = list(set(p1) | set(p2))  # Union of features\n",
    "        if len(combined) > subset_size:\n",
    "            child = sorted(random.sample(combined, subset_size))  # Ensure correct size\n",
    "        else:\n",
    "            child = sorted(combined)  # Keep all if below subset_size\n",
    "        return child\n",
    "\n",
    "    # Mutation replaces a random index in child if random.threshold is met\n",
    "    def mutation(individual, n_features, subset_size):\n",
    "        if random.random() < 0.1:  # 10% chance of mutation\n",
    "            i = random.randrange(subset_size)\n",
    "            available_features = set(range(n_features)) - set(individual)  # Exclude existing features\n",
    "            if available_features:  \n",
    "                new_feature = random.choice(list(available_features))\n",
    "                individual[i] = new_feature\n",
    "                individual.sort()\n",
    "        return individual\n",
    "\n",
    "\n",
    "    for i in range(n_generations):\n",
    "        print(f\"\\nGeneration {i+1}/{n_generations}\")\n",
    "        # Evaluate fitness of population\n",
    "        print(\"Evaluating fitness for each individual:\")\n",
    "        scored_population = []\n",
    "        for idx, ind in enumerate(population):\n",
    "            fitness_score = fitness(ind)\n",
    "            scored_population.append((fitness_score, ind))\n",
    "            print(f\"Individual {idx+1}/{len(population)}: Fitness = {fitness_score:.4f}\")\n",
    "        \n",
    "        scored_population.sort(key=lambda x: x[0], reverse=True)\n",
    "        print(f\"\\nBest fitness in generation {i+1}: {scored_population[0][0]:.4f}\")\n",
    "        \n",
    "        # Selection: truncation selection (pick top half as survivors)\n",
    "        survivors = scored_population[: population_size // 2]\n",
    "        \n",
    "        # Then randomly select two parents (p1 & p2) from survivors for crossover + mutation\n",
    "        print(\"Creating new population...\")\n",
    "        new_pop = [s[1] for s in survivors]\n",
    "        while len(new_pop) < population_size:\n",
    "            print(\"Generating new individual...\")\n",
    "            \n",
    "            p1 = random.choice(survivors)[1]\n",
    "            p2 = random.choice(survivors)[1]\n",
    "            child = crossover(p1, p2, subset_size)\n",
    "            child = mutation(child, n_features, subset_size)\n",
    "            \n",
    "            child = list(set(child))  # remove duplicates if any\n",
    "            while len(child) < subset_size:  # if duplicates reduced size\n",
    "                child.append(random.randrange(n_features))\n",
    "            child.sort()\n",
    "            new_pop.append(child)\n",
    "            \n",
    "            print(\"New individual created! Happy birthday!\")\n",
    "        population = new_pop\n",
    "        print(\"Current best:\", max([(fitness(ind), ind) for ind in population], key=lambda x: x[0])[1])\n",
    "    \n",
    "    best = max([(fitness(ind), ind) for ind in population], key=lambda x: x[0])[1]\n",
    "    return best\n",
    "\n",
    "best_features = run_genetic_algorithm(X_train_scaled, y_train)\n",
    "X_train_ga = X_train_scaled.iloc[:, best_features]\n",
    "X_test_ga = X_test_scaled.iloc[:, best_features]\n",
    "model = CatBoostClassifier(\n",
    "            iterations=500,\n",
    "            learning_rate=0.1,\n",
    "            depth=6,\n",
    "            class_weights=class_weights_dict,\n",
    "            random_seed=42,\n",
    "            verbose=100\n",
    "        )\n",
    "model.fit(X_train_ga, y_train)\n",
    "y_pred = model.predict(X_test_ga)\n",
    "print(\"Model accuracy with selected features:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO and ACO for catboost hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyswarm import pso\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split # Optional: if creating validation set inside\n",
    "\n",
    "def run_pso_for_catboost_hyperparams(X_train_ga, y_train, X_test_ga, y_test, n_particles=10, n_iterations=5):\n",
    "    \"\"\"\n",
    "    Runs Particle Swarm Optimization (PSO) to find optimal hyperparameters for CatBoost using the provided training and testing data (already filtered by GA features).\n",
    "\n",
    "    Args:\n",
    "        X_train_ga: Training features (pandas DataFrame or numpy array) selected by GA.\n",
    "        y_train: Training target variable.\n",
    "        X_test_ga: Testing features (pandas DataFrame or numpy array) selected by GA.\n",
    "        y_test: Testing target variable.\n",
    "        n_particles: Number of particles in the swarm.\n",
    "        n_iterations: Number of iterations for PSO.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_params_dict, best_score)\n",
    "               - best_params_dict: Dictionary containing the best hyperparameter set found.\n",
    "               - best_score: The best AUC score achieved by PSO.\n",
    "    \"\"\"\n",
    "    print(f\"Starting PSO with {n_particles} particles and {n_iterations} iterations...\")\n",
    "\n",
    "    # Mapping for categorical parameters (indices):\n",
    "    grow_policy_map = ['SymmetricTree', 'Depthwise', 'Lossguide']\n",
    "    bootstrap_type_map = ['Bayesian', 'Bernoulli', 'MVS']\n",
    "    leaf_estimation_method_map = ['Newton', 'Gradient']\n",
    "    boosting_type_map = ['Ordered', 'Plain']\n",
    "\n",
    "    param_bounds_pso = {\n",
    "        # Key order must match the order in objective_function_pso's params array\n",
    "        'learning_rate': (0.01, 0.3),             # params[0]\n",
    "        'depth': (3, 10),                         # params[1] Integer\n",
    "        'l2_leaf_reg': (1.0, 10.0),               # params[2]\n",
    "        'iterations': (100, 1000),                # params[3] Integer\n",
    "        'grow_policy_idx': (0, len(grow_policy_map) - 1), # params[4] Integer index\n",
    "        'max_leaves': (4, 64),                    # params[5] Integer\n",
    "        'min_data_in_leaf': (1, 50),              # params[6] Integer\n",
    "        'random_strength': (0.1, 5.0),            # params[7]\n",
    "        'bagging_temperature': (0.0, 1.0),        # params[8]\n",
    "        'bootstrap_type_idx': (0, len(bootstrap_type_map) - 1), # params[9] Integer index\n",
    "        'subsample': (0.5, 1.0),                  # params[10] Used only if bootstrap_type is not Bayesian\n",
    "        'rsm': (0.5, 1.0),                        # params[11] aka colsample_bylevel\n",
    "        'leaf_estimation_method_idx': (0, len(leaf_estimation_method_map) - 1), # params[12] Integer index\n",
    "        'leaf_estimation_iterations': (1, 10),    # params[13] Integer\n",
    "        'boosting_type_idx': (0, len(boosting_type_map) - 1), # params[14] Integer index\n",
    "        'one_hot_max_size': (2, 30),              # params[15] Integer\n",
    "        'early_stopping_rounds': (10, 50)         # params[16] Integer, used in fit\n",
    "    }\n",
    "\n",
    "    # Ordered list of keys corresponding to the parameter order\n",
    "    param_keys = list(param_bounds_pso.keys())\n",
    "\n",
    "    # Define the objective function for PSO\n",
    "    def objective_function_pso(params):\n",
    "        # --- Parameter Extraction and Mapping ---\n",
    "        depth = int(round(params[param_keys.index('depth')]))\n",
    "        iterations = int(round(params[param_keys.index('iterations')]))\n",
    "        grow_policy_idx = int(round(params[param_keys.index('grow_policy_idx')]))\n",
    "        max_leaves = int(round(params[param_keys.index('max_leaves')]))\n",
    "        min_data_in_leaf = int(round(params[param_keys.index('min_data_in_leaf')]))\n",
    "        bootstrap_type_idx = int(round(params[param_keys.index('bootstrap_type_idx')]))\n",
    "        leaf_estimation_method_idx = int(round(params[param_keys.index('leaf_estimation_method_idx')]))\n",
    "        leaf_estimation_iterations = int(round(params[param_keys.index('leaf_estimation_iterations')]))\n",
    "        boosting_type_idx = int(round(params[param_keys.index('boosting_type_idx')]))\n",
    "        one_hot_max_size = int(round(params[param_keys.index('one_hot_max_size')]))\n",
    "        early_stopping_rounds = int(round(params[param_keys.index('early_stopping_rounds')]))\n",
    "\n",
    "        grow_policy = grow_policy_map[grow_policy_idx]\n",
    "        bootstrap_type = bootstrap_type_map[bootstrap_type_idx]\n",
    "        leaf_estimation_method = leaf_estimation_method_map[leaf_estimation_method_idx]\n",
    "        boosting_type = boosting_type_map[boosting_type_idx]\n",
    "\n",
    "        # --- Build CatBoost Params ---\n",
    "        cb_params = {\n",
    "            'learning_rate': params[param_keys.index('learning_rate')],\n",
    "            'depth': depth,\n",
    "            'l2_leaf_reg': params[param_keys.index('l2_leaf_reg')],\n",
    "            'iterations': iterations,\n",
    "            'grow_policy': grow_policy,\n",
    "            'max_leaves': max_leaves,\n",
    "            'min_data_in_leaf': min_data_in_leaf,\n",
    "            'random_strength': params[param_keys.index('random_strength')],\n",
    "            'bagging_temperature': params[param_keys.index('bagging_temperature')],\n",
    "            'bootstrap_type': bootstrap_type,\n",
    "            'rsm': params[param_keys.index('rsm')], # colsample_bylevel\n",
    "            'leaf_estimation_method': leaf_estimation_method,\n",
    "            'leaf_estimation_iterations': leaf_estimation_iterations,\n",
    "            'boosting_type': boosting_type,\n",
    "            'one_hot_max_size': one_hot_max_size,\n",
    "            'loss_function': 'Logloss',\n",
    "            'eval_metric': 'AUC',\n",
    "            'verbose': 0,\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        if bootstrap_type in ['Bernoulli', 'MVS']:\n",
    "            cb_params['subsample'] = params[param_keys.index('subsample')]\n",
    "        # --- Optional: Create Validation Set ---\n",
    "        # If you want to use early stopping on a validation set instead of the test set:\n",
    "        # X_train_part, X_val_part, y_train_part, y_val_part = train_test_split(\n",
    "        #     X_train_ga, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        # )\n",
    "        # eval_set_data = (X_val_part, y_val_part)\n",
    "\n",
    "        # --- Train and Evaluate ---\n",
    "        model = CatBoostClassifier(**cb_params)\n",
    "\n",
    "        try:\n",
    "            # Using X_test_ga as eval set for early stopping\n",
    "            model.fit(X_train_ga, y_train,\n",
    "                      eval_set=(X_test_ga, y_test), # Or use eval_set_data if using validation split\n",
    "                      early_stopping_rounds=early_stopping_rounds,\n",
    "                      verbose=0)\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test_ga)[:, 1]\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Exception during model training/evaluation: {e}\")\n",
    "            print(f\"Params causing issue: {cb_params}\")\n",
    "            # Return a very low score for problematic parameter combinations\n",
    "            auc = 0.0\n",
    "\n",
    "        # PSO maximizes, so return AUC directly\n",
    "        return auc\n",
    "\n",
    "    # Extract bounds for pyswarm\n",
    "    lb = [param_bounds_pso[k][0] for k in param_keys]\n",
    "    ub = [param_bounds_pso[k][1] for k in param_keys]\n",
    "\n",
    "    # Run PSO\n",
    "    best_params_pso_vals, best_score_pso = pso(objective_function_pso, lb, ub, swarmsize=n_particles, maxiter=n_iterations)\n",
    "\n",
    "    # --- Process Results ---\n",
    "    # Map best_params_pso_vals back to dictionary\n",
    "    best_params_pso_dict = {}\n",
    "    for i, key in enumerate(param_keys):\n",
    "        val = best_params_pso_vals[i]\n",
    "        if key in ['depth', 'iterations', 'grow_policy_idx', 'max_leaves', 'min_data_in_leaf',\n",
    "                   'bootstrap_type_idx', 'leaf_estimation_method_idx', 'leaf_estimation_iterations',\n",
    "                   'boosting_type_idx', 'one_hot_max_size', 'early_stopping_rounds']:\n",
    "            best_params_pso_dict[key] = int(round(val))\n",
    "        else:\n",
    "            best_params_pso_dict[key] = val\n",
    "\n",
    "    # Map indices back for categorical features\n",
    "    best_params_pso_dict['grow_policy'] = grow_policy_map[best_params_pso_dict['grow_policy_idx']]\n",
    "    best_params_pso_dict['bootstrap_type'] = bootstrap_type_map[best_params_pso_dict['bootstrap_type_idx']]\n",
    "    best_params_pso_dict['leaf_estimation_method'] = leaf_estimation_method_map[best_params_pso_dict['leaf_estimation_method_idx']]\n",
    "    best_params_pso_dict['boosting_type'] = boosting_type_map[best_params_pso_dict['boosting_type_idx']]\n",
    "\n",
    "    # Store the early stopping rounds value separately as it's used in fit, not constructor\n",
    "    best_early_stopping_rounds = best_params_pso_dict['early_stopping_rounds']\n",
    "\n",
    "    # Remove index keys and early stopping rounds from the main dict\n",
    "    keys_to_remove = ['grow_policy_idx', 'bootstrap_type_idx', 'leaf_estimation_method_idx', 'boosting_type_idx', 'early_stopping_rounds']\n",
    "    for key in keys_to_remove:\n",
    "       best_params_pso_dict.pop(key, None)\n",
    "\n",
    "    # Handle conditional subsample: Remove if not needed, ensure exists if needed\n",
    "    if best_params_pso_dict['bootstrap_type'] not in ['Bernoulli', 'MVS']:\n",
    "        best_params_pso_dict.pop('subsample', None)\n",
    "    elif 'subsample' not in best_params_pso_dict:\n",
    "         # If needed but missing (e.g., boundary issue), assign a default or average\n",
    "         best_params_pso_dict['subsample'] = np.mean(param_bounds_pso['subsample'])\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"PSO Finished.\")\n",
    "    print(f\"Best AUC score found by PSO: {best_score_pso:.6f}\")\n",
    "    print(\"Best CatBoost Parameters (excluding early_stopping_rounds):\")\n",
    "    for key, val in best_params_pso_dict.items():\n",
    "        print(f\"  {key}: {val}\")\n",
    "    print(f\"Best early_stopping_rounds: {best_early_stopping_rounds}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Add early stopping rounds back for potential direct use later if needed\n",
    "    best_params_pso_dict_final = best_params_pso_dict.copy()\n",
    "    best_params_pso_dict_final['early_stopping_rounds'] = best_early_stopping_rounds\n",
    "\n",
    "    return best_params_pso_dict_final, best_score_pso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split # Optional\n",
    "\n",
    "class ACO_HyperparameterOptimizer:\n",
    "    \"\"\"\n",
    "    Ant Colony Optimization for Hyperparameter Tuning.\n",
    "\n",
    "    Attributes:\n",
    "        param_grid (dict): Dictionary where keys are hyperparameter names\n",
    "                           and values are lists of discrete values to explore.\n",
    "        objective_function (callable): A function that takes a dictionary of\n",
    "                                      hyperparameters and returns a score to maximize.\n",
    "        n_ants (int): Number of ants (solutions generated per iteration).\n",
    "        n_iterations (int): Number of optimization iterations.\n",
    "        alpha (float): Pheromone influence factor.\n",
    "        beta (float): Heuristic influence factor (currently unused, set to 1).\n",
    "        rho (float): Pheromone evaporation rate (0 < rho <= 1).\n",
    "        Q (float): Pheromone deposit constant.\n",
    "        min_pheromone (float): Minimum pheromone level to prevent stagnation.\n",
    "        pheromones (dict): Nested dictionary storing pheromone levels for each parameter choice.\n",
    "        global_best_score (float): Best score found across all iterations.\n",
    "        global_best_params (dict): Hyperparameter set corresponding to the best score.\n",
    "    \"\"\"\n",
    "    def __init__(self, param_grid, objective_function, n_ants, n_iterations,\n",
    "                 alpha=1.0, beta=1.0, rho=0.1, Q=1.0, min_pheromone=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the ACO optimizer.\n",
    "        \"\"\"\n",
    "        self.param_grid = param_grid\n",
    "        self.objective_function = objective_function\n",
    "        self.n_ants = n_ants\n",
    "        self.n_iterations = n_iterations\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta  # Heuristic factor (often kept simple for HP tuning)\n",
    "        self.rho = rho    # Evaporation rate\n",
    "        self.Q = Q        # Pheromone deposit constant\n",
    "        self.min_pheromone = min_pheromone # Minimum pheromone level\n",
    "\n",
    "        self.param_names = list(self.param_grid.keys())\n",
    "        self.pheromones = self._initialize_pheromones()\n",
    "\n",
    "        self.global_best_score = -np.inf\n",
    "        self.global_best_params = None\n",
    "\n",
    "    def _initialize_pheromones(self):\n",
    "        \"\"\"\n",
    "        Initializes pheromone trails with a constant value for all parameter choices.\n",
    "        \"\"\"\n",
    "        pheromones = {}\n",
    "        initial_pheromone = 1.0 # Start with a neutral pheromone level\n",
    "        for param, values in self.param_grid.items():\n",
    "            pheromones[param] = {value: initial_pheromone for value in values}\n",
    "        return pheromones\n",
    "\n",
    "    def _select_next_node(self, param_name):\n",
    "        \"\"\"\n",
    "        Selects a value for a given parameter based on pheromone levels.\n",
    "        Uses a probabilistic choice mechanism influenced by pheromones.\n",
    "        \"\"\"\n",
    "        pheromone_values = self.pheromones[param_name]\n",
    "        choices = list(pheromone_values.keys())\n",
    "        current_pheromones = np.array([pheromone_values[choice] for choice in choices])\n",
    "\n",
    "        # Heuristic information (tau^alpha * eta^beta)\n",
    "        # For hyperparameter tuning, heuristic info (eta) is often uniform (beta=0 or eta=1)\n",
    "        # unless prior knowledge exists. We'll use beta=1 but eta=1 implicitly.\n",
    "        selection_probs = current_pheromones ** self.alpha # * (heuristic ** self.beta)\n",
    "\n",
    "        # Handle potential division by zero if all probs are zero (e.g., early stage)\n",
    "        prob_sum = np.sum(selection_probs)\n",
    "        if prob_sum == 0:\n",
    "            # If sum is zero, choose uniformly\n",
    "            selection_probs = np.ones(len(choices)) / len(choices)\n",
    "        else:\n",
    "            selection_probs = selection_probs / prob_sum\n",
    "\n",
    "        # Choose a value based on calculated probabilities\n",
    "        chosen_value = np.random.choice(choices, p=selection_probs)\n",
    "        return chosen_value\n",
    "\n",
    "    def _construct_solution(self):\n",
    "        \"\"\"\n",
    "        Constructs a complete hyperparameter set (solution) for a single ant.\n",
    "        \"\"\"\n",
    "        solution = {}\n",
    "        for param_name in self.param_names:\n",
    "            solution[param_name] = self._select_next_node(param_name)\n",
    "        return solution\n",
    "\n",
    "    def _update_pheromones(self, ant_solutions, ant_scores):\n",
    "        \"\"\"\n",
    "        Updates pheromone levels based on evaporation and deposition from ant solutions.\n",
    "        \"\"\"\n",
    "        # 1. Evaporation\n",
    "        for param, values in self.pheromones.items():\n",
    "            for value in values:\n",
    "                self.pheromones[param][value] *= (1.0 - self.rho)\n",
    "                # Enforce minimum pheromone level\n",
    "                self.pheromones[param][value] = max(self.pheromones[param][value], self.min_pheromone)\n",
    "\n",
    "        # 2. Deposition\n",
    "        # Deposit pheromone based on the quality of solutions found\n",
    "        # More sophisticated strategies exist (e.g., only best ant deposits, elitism)\n",
    "        # Here, all ants deposit based on their score\n",
    "        for i in range(self.n_ants):\n",
    "            solution = ant_solutions[i]\n",
    "            score = ant_scores[i]\n",
    "\n",
    "            # Normalize score or use directly? Depends on score range.\n",
    "            # Assuming score is something like AUC (0 to 1), direct use might be okay.\n",
    "            # We use Q as a scaling factor.\n",
    "            if score > -np.inf: # Only deposit if the evaluation was successful\n",
    "                delta_pheromone = self.Q * score # Simple proportional deposit\n",
    "\n",
    "                for param_name, value in solution.items():\n",
    "                    # Check if the parameter/value exists (it should)\n",
    "                    if param_name in self.pheromones and value in self.pheromones[param_name]:\n",
    "                        self.pheromones[param_name][value] += delta_pheromone\n",
    "                    else:\n",
    "                        # This case should ideally not happen if grid is consistent\n",
    "                         print(f\"Warning: Parameter '{param_name}' or value '{value}' not found in pheromone trails during deposition.\")\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Executes the ACO optimization process.\n",
    "        \"\"\"\n",
    "        print(f\"Starting ACO: {self.n_iterations} iterations, {self.n_ants} ants/iteration.\")\n",
    "        print(f\"Params: alpha={self.alpha}, beta={self.beta}, rho={self.rho}, Q={self.Q}\")\n",
    "\n",
    "        for iteration in range(self.n_iterations):\n",
    "            ant_solutions = []\n",
    "            ant_scores = []\n",
    "\n",
    "            # Generate solutions for all ants\n",
    "            for ant in range(self.n_ants):\n",
    "                solution = self._construct_solution()\n",
    "                try:\n",
    "                    score = self.objective_function(solution.copy()) # Pass a copy\n",
    "                    if score is None or not np.isfinite(score):\n",
    "                       # Handle cases where objective function fails or returns invalid score\n",
    "                       print(f\"Warning: Objective function returned invalid score ({score}) for params: {solution}. Assigning low score.\")\n",
    "                       score = -np.inf\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating solution: {solution}\")\n",
    "                    print(f\"Exception: {e}\")\n",
    "                    score = -np.inf # Penalize errors heavily\n",
    "\n",
    "                ant_solutions.append(solution)\n",
    "                ant_scores.append(score)\n",
    "\n",
    "                # Update global best if current ant is better\n",
    "                if score > self.global_best_score:\n",
    "                    self.global_best_score = score\n",
    "                    self.global_best_params = solution\n",
    "                    print(f\"Iteration {iteration+1}, Ant {ant+1}: New best score! -> {self.global_best_score:.6f}\")\n",
    "\n",
    "            # Update pheromone trails based on the iteration's results\n",
    "            self._update_pheromones(ant_solutions, ant_scores)\n",
    "\n",
    "            avg_score = np.mean([s for s in ant_scores if s > -np.inf]) if any(s > -np.inf for s in ant_scores) else -np.inf\n",
    "            print(f\"Iteration {iteration+1}/{self.n_iterations} finished. Avg Score: {avg_score:.6f}. Best Score so far: {self.global_best_score:.6f}\")\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "        print(\"ACO Finished.\")\n",
    "        if self.global_best_params:\n",
    "            print(f\"Best score found: {self.global_best_score:.6f}\")\n",
    "            print(\"Best parameters found:\")\n",
    "            for param, value in self.global_best_params.items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "        else:\n",
    "            print(\"ACO did not find a valid solution.\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        return self.global_best_params, self.global_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries (CatBoostClassifier, roc_auc_score, etc.)\n",
    "# Make sure the ACO_HyperparameterOptimizer class definition is above this function\n",
    "\n",
    "def run_aco_for_catboost_hyperparams(X_train_ga, y_train, X_test_ga, y_test,\n",
    "                                     n_ants=10, n_iterations=5,\n",
    "                                     alpha=1.0, beta=1.0, rho=0.1, Q=1.0): # Add ACO params\n",
    "    \"\"\"\n",
    "    Runs Ant Colony Optimization (ACO) using the implemented class to find\n",
    "    optimal hyperparameters for CatBoost using the provided GA-filtered data.\n",
    "\n",
    "    Args:\n",
    "        X_train_ga: Training features (pandas DataFrame or numpy array) selected by GA.\n",
    "        y_train: Training target variable.\n",
    "        X_test_ga: Testing features (pandas DataFrame or numpy array) selected by GA.\n",
    "        y_test: Testing target variable.\n",
    "        n_ants (int): Number of ants (solutions per iteration).\n",
    "        n_iterations (int): Number of iterations for ACO.\n",
    "        alpha (float): Pheromone influence factor for ACO.\n",
    "        beta (float): Heuristic influence factor for ACO (currently unused).\n",
    "        rho (float): Pheromone evaporation rate for ACO.\n",
    "        Q (float): Pheromone deposit constant for ACO.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_params_dict, best_score)\n",
    "               - best_params_dict: Dictionary containing the best hyperparameter set found.\n",
    "               - best_score: The best AUC score achieved by ACO.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Define Parameter Grid (same as before) ---\n",
    "    param_grid_aco = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.3],\n",
    "        'depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'l2_leaf_reg': [1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "        'iterations': [100, 200, 300, 500, 700, 1000],\n",
    "        'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide'],\n",
    "        'max_leaves': [8, 16, 32, 64],\n",
    "        'min_data_in_leaf': [1, 5, 10, 20, 50],\n",
    "        'random_strength': [0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "        'bagging_temperature': [0.0, 0.2, 0.5, 0.8, 1.0],\n",
    "        'bootstrap_type': ['Bayesian', 'Bernoulli', 'MVS'],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # Used only if bootstrap_type is not Bayesian\n",
    "        'rsm': [0.6, 0.7, 0.8, 0.9, 1.0], # colsample_bylevel\n",
    "        'leaf_estimation_method': ['Newton', 'Gradient'],\n",
    "        'leaf_estimation_iterations': [1, 3, 5, 10],\n",
    "        'boosting_type': ['Ordered', 'Plain'],\n",
    "        'one_hot_max_size': [2, 10, 20, 30],\n",
    "        'early_stopping_rounds': [10, 20, 30, 50] # Used in fit\n",
    "    }\n",
    "\n",
    "    # --- Define Objective Function (same as before) ---\n",
    "    def objective_function_aco(params):\n",
    "        # params is a dictionary constructed by the ACO framework for one ant\n",
    "        cb_params = params.copy()\n",
    "        early_stopping_rounds = cb_params.pop('early_stopping_rounds', 30) # Default if missing\n",
    "\n",
    "        bootstrap_type = cb_params.get('bootstrap_type', 'Bayesian')\n",
    "        if bootstrap_type not in ['Bernoulli', 'MVS']:\n",
    "            cb_params.pop('subsample', None)\n",
    "        elif 'subsample' not in cb_params:\n",
    "            # Handle missing subsample if required by bootstrap_type\n",
    "            # Give it a default value from the grid if ACO didn't pick it\n",
    "             median_subsample = np.median(param_grid_aco['subsample'])\n",
    "             # print(f\"Warning: Subsample needed for {bootstrap_type} but not chosen by ant. Setting to median: {median_subsample}\")\n",
    "             cb_params['subsample'] = median_subsample\n",
    "\n",
    "\n",
    "        cb_params.update({\n",
    "            'loss_function': 'Logloss',\n",
    "            'eval_metric': 'AUC',\n",
    "            'verbose': 0,\n",
    "            'random_state': 42\n",
    "        })\n",
    "\n",
    "        # --- Optional: Validation Split ---\n",
    "        # X_train_part, X_val_part, y_train_part, y_val_part = train_test_split(...)\n",
    "        # eval_set_data = (X_val_part, y_val_part)\n",
    "\n",
    "        model = CatBoostClassifier(**cb_params)\n",
    "\n",
    "        try:\n",
    "            model.fit(X_train_ga, y_train,\n",
    "                      eval_set=(X_test_ga, y_test), # Or use eval_set_data\n",
    "                      early_stopping_rounds=early_stopping_rounds,\n",
    "                      verbose=0)\n",
    "            y_pred_proba = model.predict_proba(X_test_ga)[:, 1]\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            # Ensure we don't return NaN/inf scores which break ACO updates\n",
    "            if not np.isfinite(auc):\n",
    "                auc = 0.0 # Or some other low penalty score\n",
    "        except Exception as e:\n",
    "            # print(f\"Warning: Exception during model training/evaluation: {e}\")\n",
    "            # print(f\"Params causing issue: {cb_params}\")\n",
    "            auc = 0.0 # Penalize errors\n",
    "\n",
    "        return auc\n",
    "\n",
    "    # --- Instantiate and Run ACO ---\n",
    "    aco_optimizer = ACO_HyperparameterOptimizer(\n",
    "        param_grid=param_grid_aco,\n",
    "        objective_function=objective_function_aco,\n",
    "        n_ants=n_ants,\n",
    "        n_iterations=n_iterations,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        rho=rho,\n",
    "        Q=Q\n",
    "    )\n",
    "\n",
    "    best_params_aco, best_score_aco = aco_optimizer.run() # Run the optimization\n",
    "\n",
    "    # --- Process and Return Results ---\n",
    "    # The run method already prints results. We just return them.\n",
    "    # Note: best_params_aco will include 'early_stopping_rounds' if found.\n",
    "    return best_params_aco, best_score_aco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PSO and ACO optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PSO optimization\n",
    "print(\"Starting PSO optimization...\")\n",
    "best_pso_params, best_pso_score = run_pso_for_catboost_hyperparams(\n",
    "    X_train_ga, y_train, X_test_ga, y_test,\n",
    "    n_particles=10,\n",
    "    n_iterations=5\n",
    ")\n",
    "\n",
    "# Run ACO optimization\n",
    "print(\"\\nStarting ACO optimization...\")\n",
    "best_aco_params, best_aco_score = run_aco_for_catboost_hyperparams(\n",
    "    X_train_ga, y_train, X_test_ga, y_test,\n",
    "    n_ants=10,\n",
    "    n_iterations=5,\n",
    "    alpha=1.0,\n",
    "    beta=1.0,\n",
    "    rho=0.1,\n",
    "    Q=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate final models with optimized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with PSO parameters\n",
    "final_esr_pso = best_pso_params.pop('early_stopping_rounds')\n",
    "final_model_pso = CatBoostClassifier(\n",
    "    **best_pso_params,\n",
    "    random_state=42,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC'\n",
    ")\n",
    "final_model_pso.fit(\n",
    "    X_train_ga, y_train,\n",
    "    eval_set=(X_test_ga, y_test),\n",
    "    early_stopping_rounds=final_esr_pso,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Train final model with ACO parameters\n",
    "final_esr_aco = best_aco_params.pop('early_stopping_rounds')\n",
    "final_model_aco = CatBoostClassifier(\n",
    "    **best_aco_params,\n",
    "    random_state=42,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC'\n",
    ")\n",
    "final_model_aco.fit(\n",
    "    X_train_ga, y_train,\n",
    "    eval_set=(X_test_ga, y_test),\n",
    "    early_stopping_rounds=final_esr_aco,\n",
    "    verbose=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare PSO and ACO results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_and_visualize(model, X_train, X_test, y_train, y_test, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with visualizations\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model object\n",
    "        X_train: Training features\n",
    "        X_test: Test features\n",
    "        y_train: Training labels\n",
    "        y_test: Test labels\n",
    "        title_prefix: String to prepend to plot titles\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing various performance metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    avg_precision = metrics.average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # 1. Print Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred, digits=4))\n",
    "    \n",
    "    # 2. Plot Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Not Fraud', 'Fraud'],\n",
    "                yticklabels=['Not Fraud', 'Fraud'])\n",
    "    plt.title(f'{title_prefix}Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. ROC Curve\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "             label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{title_prefix}Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Precision-Recall Curve\n",
    "    precision, recall, _ = metrics.precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='blue', lw=2,\n",
    "             label=f'P-R curve (AP = {avg_precision:.4f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{title_prefix}Precision-Recall Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Feature Importance Plot (top 20)\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.get_feature_importance()\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=feature_importance.head(20),\n",
    "                x='importance', y='feature', palette='viridis')\n",
    "    plt.title(f'{title_prefix}Top 20 Feature Importance')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. Learning Curves\n",
    "    history = pd.DataFrame(model.get_evals_result()['validation'])\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for metric in history.columns:\n",
    "        plt.plot(history.index, history[metric], label=metric)\n",
    "    plt.title(f'{title_prefix}Learning Curves')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics dictionary\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc,\n",
    "        'avg_precision': avg_precision,\n",
    "        'confusion_matrix': cm,\n",
    "        'f1_score': metrics.f1_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Evaluate PSO model\n",
    "print(\"Evaluating PSO-tuned model:\")\n",
    "metrics_pso = evaluate_and_visualize(\n",
    "    final_model_pso,\n",
    "    X_train_ga, X_test_ga,\n",
    "    y_train, y_test,\n",
    "    title_prefix=\"PSO-Tuned CatBoost: \"\n",
    ")\n",
    "\n",
    "# Evaluate ACO model\n",
    "print(\"\\nEvaluating ACO-tuned model:\")\n",
    "metrics_aco = evaluate_and_visualize(\n",
    "    final_model_aco,\n",
    "    X_train_ga, X_test_ga,\n",
    "    y_train, y_test,\n",
    "    title_prefix=\"ACO-Tuned CatBoost: \"\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "results_comparison = pd.DataFrame({\n",
    "    'PSO': metrics_pso,\n",
    "    'ACO': metrics_aco\n",
    "}).round(4)\n",
    "print(\"\\nPSO vs ACO Performance Comparison:\")\n",
    "print(results_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publication-Quality Visualizations for PSO and ACO Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set style for publication-quality plots\n",
    "plt.style.use('seaborn-paper')\n",
    "sns.set_context(\"paper\", font_scale=1.5)\n",
    "\n",
    "# Custom color palette\n",
    "colors = ['#2ecc71', '#e74c3c']  # Green for PSO, Red for ACO\n",
    "sns.set_palette(colors)\n",
    "\n",
    "# 1. ROC Curves Comparison\n",
    "plt.figure(figsize=(10, 8))\n",
    "for model, name, color in [(final_model_pso, 'PSO', colors[0]), \n",
    "                          (final_model_aco, 'ACO', colors[1])]:\n",
    "    y_pred_proba = model.predict_proba(X_test_ga)[:, 1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=color, lw=2,\n",
    "             label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison: PSO vs ACO')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Precision-Recall Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for model, name, color in [(final_model_pso, 'PSO', colors[0]), \n",
    "                          (final_model_aco, 'ACO', colors[1])]:\n",
    "    y_pred_proba = model.predict_proba(X_test_ga)[:, 1]\n",
    "    precision, recall, _ = metrics.precision_recall_curve(y_test, y_pred_proba)\n",
    "    avg_precision = metrics.average_precision_score(y_test, y_pred_proba)\n",
    "    plt.plot(recall, precision, color=color, lw=2,\n",
    "             label=f'{name} (AP = {avg_precision:.3f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves: PSO vs ACO')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pr_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. Feature Importance Comparison\n",
    "def plot_feature_importance_comparison(pso_model, aco_model, X_train, top_n=15):\n",
    "    # Get feature importances\n",
    "    pso_importances = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': pso_model.get_feature_importance(),\n",
    "        'model': 'PSO'\n",
    "    })\n",
    "    \n",
    "    aco_importances = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': aco_model.get_feature_importance(),\n",
    "        'model': 'ACO'\n",
    "    })\n",
    "    \n",
    "    # Combine and get top features\n",
    "    all_importances = pd.concat([pso_importances, aco_importances])\n",
    "    top_features = all_importances.groupby('feature')['importance'].mean().nlargest(top_n).index\n",
    "    \n",
    "    # Filter for top features\n",
    "    plot_data = all_importances[all_importances['feature'].isin(top_features)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=plot_data, x='importance', y='feature', hue='model')\n",
    "    plt.title(f'Top {top_n} Feature Importance Comparison')\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.legend(title='')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance_comparison(final_model_pso, final_model_aco, X_train_ga)\n",
    "\n",
    "# 4. Learning Curves Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "pso_history = pd.DataFrame(final_model_pso.get_evals_result()['validation'])\n",
    "aco_history = pd.DataFrame(final_model_aco.get_evals_result()['validation'])\n",
    "\n",
    "plt.plot(pso_history.index, pso_history['AUC'], label='PSO', color=colors[0])\n",
    "plt.plot(aco_history.index, aco_history['AUC'], label='ACO', color=colors[1])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.title('Learning Curves: PSO vs ACO')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curves_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 5. Performance Metrics Table\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Metric': ['AUC-ROC', 'Average Precision', 'Accuracy', 'F1-Score'],\n",
    "    'PSO': [metrics_pso['roc_auc'], metrics_pso['avg_precision'], \n",
    "            metrics.accuracy_score(y_test, final_model_pso.predict(X_test_ga)),\n",
    "            metrics.f1_score(y_test, final_model_pso.predict(X_test_ga))],\n",
    "    'ACO': [metrics_aco['roc_auc'], metrics_aco['avg_precision'],\n",
    "            metrics.accuracy_score(y_test, final_model_aco.predict(X_test_ga)),\n",
    "            metrics.f1_score(y_test, final_model_aco.predict(X_test_ga))]\n",
    "})\n",
    "\n",
    "# Create a styled table\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.axis('off')\n",
    "table = plt.table(cellText=metrics_comparison.values,\n",
    "                 colLabels=metrics_comparison.columns,\n",
    "                 cellLoc='center',\n",
    "                 loc='center',\n",
    "                 colColours=['#f8f9fa'] * 3)\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1.2, 1.8)\n",
    "plt.title('Performance Metrics Comparison', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_table.pdf', dpi=300, bbox_inches='tight', \n",
    "            transparent=True)\n",
    "plt.show()\n",
    "\n",
    "# 6. Statistical Significance Testing\n",
    "def bootstrap_comparison(pso_model, aco_model, X_test, y_test, n_iterations=1000):\n",
    "    pso_scores = []\n",
    "    aco_scores = []\n",
    "    n_samples = len(y_test)\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        # Bootstrap sampling\n",
    "        indices = np.random.randint(0, n_samples, n_samples)\n",
    "        X_bootstrap = X_test.iloc[indices]\n",
    "        y_bootstrap = y_test.iloc[indices]\n",
    "        \n",
    "        # Get predictions and compute AUC\n",
    "        pso_pred = pso_model.predict_proba(X_bootstrap)[:, 1]\n",
    "        aco_pred = aco_model.predict_proba(X_bootstrap)[:, 1]\n",
    "        \n",
    "        pso_scores.append(metrics.roc_auc_score(y_bootstrap, pso_pred))\n",
    "        aco_scores.append(metrics.roc_auc_score(y_bootstrap, aco_pred))\n",
    "    \n",
    "    # Statistical test\n",
    "    t_stat, p_value = stats.ttest_rel(pso_scores, aco_scores)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(data=pd.DataFrame({'PSO': pso_scores, 'ACO': aco_scores}))\n",
    "    plt.title('Bootstrap Distribution of AUC Scores\\n' + \n",
    "              f'p-value: {p_value:.4f}')\n",
    "    plt.xlabel('AUC Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bootstrap_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return t_stat, p_value\n",
    "\n",
    "t_stat, p_value = bootstrap_comparison(final_model_pso, final_model_aco, \n",
    "                                     X_test_ga, y_test)\n",
    "print(f\"\\nStatistical Comparison Results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
