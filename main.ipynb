{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Classifiers\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Additional encoders\n",
    "import category_encoders as ce\n",
    "\n",
    "# Stats\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import (\n",
    "    PCA,\n",
    "    KernelPCA, \n",
    "    FastICA,\n",
    "    TruncatedSVD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First and foremost, merge two dbs into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv(\"data/train_transaction.csv\")\n",
    "train_identity = pd.read_csv(\"data/train_identity.csv\")\n",
    "\n",
    "# Merge both dataframes on 'TransactionID'\n",
    "train = pd.merge(train_transaction, train_identity, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "print(f\"Rows in merged training set: {train.shape[0]}\")\n",
    "print(f\"Columns in merged training set: {train.shape[1]}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_identity = pd.read_csv(\"data/test_identity.csv\")\n",
    "test_transaction = pd.read_csv(\"data/test_transaction.csv\")\n",
    "test = pd.merge(test_transaction, test_identity, on=\"TransactionID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns = test.columns.str.replace('-', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform an initial exploratory data analysis (EDA) by checking missing value percentages and examining the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the target variable distribution and missing values for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value percentages per column\n",
    "missing_percent = (train.isnull().sum() / len(train)) * 100\n",
    "missing_percent = missing_percent.sort_values(ascending=False)\n",
    "print(\"Missing percentages per column:\")\n",
    "print(missing_percent[missing_percent > 0])\n",
    "\n",
    "# Distribution of the target variable 'isFraud'\n",
    "train['isFraud'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Distribution of Fraudulent vs Non-Fraudulent Transactions\")\n",
    "plt.xlabel(\"isFraud\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cols = [col for col in train.columns if train[col].isna().sum() > 0.9 * len(train)]\n",
    "null_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = train.copy(deep=True)\n",
    "for col in null_cols:\n",
    "    missing_df[\"m_flag_\"+col] = np.where(missing_df[col].isnull(), 1, 0)\n",
    "    correlation = missing_df[[\"m_flag_\"+col, 'isFraud']].corr()\n",
    "    print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = train.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_features:\n",
    "    print(col, len(set(train[col])), set(train[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find truly categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Screen resolution values are true numerical values, while all other features are categorical in nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Screen_Width', 'Screen_Height']] = train['id_33'].str.split('x', expand=True).astype(float)\n",
    "train = train.drop(columns=['id_33'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['Screen_Width', 'Screen_Height']] = test['id_33'].str.split('x', expand=True).astype(float)\n",
    "test = test.drop(columns=['id_33'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Identify candidate categorical features based on unique value counts\n",
    "candidate_categorical = {}\n",
    "# Set a threshold for maximum unique values\n",
    "unique_threshold = 20\n",
    "\n",
    "# Iterate over numeric columns to check unique value counts\n",
    "for col in train.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    unique_vals = train[col].nunique()\n",
    "    if (unique_vals < unique_threshold) and (col != \"isFraud\"):\n",
    "        candidate_categorical[col] = unique_vals\n",
    "\n",
    "# Print candidate categorical features\n",
    "print(\"Candidate categorical features (numeric columns with few unique values):\")\n",
    "for col, count in candidate_categorical.items():\n",
    "    print(f\"{col}: {count} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not using standard imputation:\n",
    "1. Placed zero values as indicator for missing values where feature values no zero values anywhere else\n",
    "2. Added 'missing' instead of null for categorical values to keep all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical columns\n",
    "num_cols = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = list(set(cat_cols).union(set(candidate_categorical.keys())))\n",
    "num_cols = [col for col in num_cols if col not in cat_cols and col not in (\"TransactionID\", \"isFraud\")]\n",
    "\n",
    "# Imputation for numeric columns using zeros as indicator\n",
    "num_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "train[num_cols] = num_imputer.fit_transform(train[num_cols])\n",
    "\n",
    "# Imputation for categorical columns using a constant value\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "train[cat_cols] = cat_imputer.fit_transform(train[cat_cols])\n",
    "\n",
    "# Confirm that no missing values remain (or check overall missing count)\n",
    "print(\"Total missing values after imputation:\", train.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[num_cols] = num_imputer.transform(test[num_cols])\n",
    "test[cat_cols] = cat_imputer.transform(test[cat_cols])\n",
    "print(\"Total missing values after imputation:\", train.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=[\"isFraud\", \"TransactionID\"])\n",
    "y = train['isFraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting train and test asap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using novel thing: WOE encoder to compensate for an enormous dimentionality for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_high = ce.WOEEncoder(cols=cat_cols)\n",
    "X_train_encoded_cat = encoder_high.fit_transform(X_train[cat_cols], y_train)\n",
    "X_test_encoded_cat = encoder_high.transform(X_test[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded_cat = encoder_high.transform(test[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data to remove any disrepancies in SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled_num = scaler.fit_transform(X_train[num_cols])\n",
    "X_test_scaled_num = scaler.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled_num = scaler.transform(test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(np.hstack([X_train_encoded_cat.values, X_train_scaled_num]), columns=cat_cols + num_cols)\n",
    "X_test_scaled = pd.DataFrame(np.hstack([X_test_encoded_cat.values, X_test_scaled_num]), columns=cat_cols + num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled = pd.DataFrame(np.hstack([test_encoded_cat.values, test_scaled_num]), columns=cat_cols + num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoded training set shape:\", X_train_scaled.shape)\n",
    "print(\"Encoded test set shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoded final test set shape:\", test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled = pd.DataFrame(np.hstack([test[[\"TransactionID\"]], test_scaled]), columns=[\"TransactionID\"] + list(test_scaled.columns))\n",
    "test_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.to_csv(\"data/preprocessed_train.csv\", index=False)\n",
    "X_test_scaled.to_csv(\"data/preprocessed_test.csv\", index=False)\n",
    "\n",
    "y_train.to_csv(\"data/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"data/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled.to_csv(\"data/test_for_model_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.read_csv(\"data/preprocessed_train.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/preprocessed_test.csv\")\n",
    "y_train = pd.read_csv(\"data/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accounting for severe class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train to numpy array (since it's read as DataFrame)\n",
    "y_train_array = y_train['isFraud'].values\n",
    "\n",
    "# Get unique classes and compute weights\n",
    "unique_classes = np.unique(y_train_array)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=unique_classes,\n",
    "    y=y_train_array\n",
    ")\n",
    "\n",
    "# Create dictionary of class weights\n",
    "class_weights_dict = dict(zip(unique_classes, class_weights))\n",
    "class_weights_dict = {int(k): float(v) for k, v in class_weights_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30)\n",
    "X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent Component Analysis (ICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components=30, random_state=42)\n",
    "X_train_scaled = ica.fit_transform(X_train_scaled)\n",
    "X_test_scaled = ica.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=30, random_state=42)\n",
    "X_train_scaled = svd.fit_transform(X_train_scaled)\n",
    "X_test_scaled = svd.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_flat = y_train.values.ravel()\n",
    "y_test_flat = y_test.values.ravel()\n",
    "\n",
    "model_cat = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    class_weights=class_weights_dict,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "model_cat.fit(X_train_scaled, y_train_flat)\n",
    "y_pred_cat = model_cat.predict(X_test_scaled)\n",
    "\n",
    "print(\"CatBoost Training Accuracy:\", accuracy_score(y_test_flat, y_pred_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With all features:\n",
    "0:\tlearn: 0.6515508\ttotal: 127ms\tremaining: 1m 3s \n",
    "\n",
    "100:\tlearn: 0.3743155\ttotal: 13.5s\tremaining: 53.4s\n",
    "\n",
    "200:\tlearn: 0.3282363\ttotal: 26.9s\tremaining: 40s\n",
    "\n",
    "300:\tlearn: 0.2981647\ttotal: 40s\tremaining: 26.5s\n",
    "\n",
    "400:\tlearn: 0.2745897\ttotal: 53.3s\tremaining: 13.2s\n",
    "\n",
    "499:\tlearn: 0.2570046\ttotal: 1m 6s\tremaining: 0us\n",
    "\n",
    "CatBoost Training Accuracy: **0.9161614793240085**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With PCA:\n",
    "0:\tlearn: 0.6556789\ttotal: 83.4ms\tremaining: 41.6s\n",
    "\n",
    "100:\tlearn: 0.4566328\ttotal: 2.05s\tremaining: 8.11s\n",
    "\n",
    "200:\tlearn: 0.4297046\ttotal: 4.63s\tremaining: 6.88s\n",
    "\n",
    "300:\tlearn: 0.4090670\ttotal: 7.42s\tremaining: 4.9s\n",
    "\n",
    "400:\tlearn: 0.3936705\ttotal: 9.96s\tremaining: 2.46s\n",
    "\n",
    "499:\tlearn: 0.3800877\ttotal: 12.6s\tremaining: 0us\n",
    "\n",
    "CatBoost Training Accuracy: **0.8490026077827073**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With ICA:\n",
    "0:\tlearn: 0.6656583\ttotal: 22ms\tremaining: 11s\n",
    "]\n",
    "100:\tlearn: 0.4502845\ttotal: 2.46s\tremaining: 9.73s\n",
    "\n",
    "200:\tlearn: 0.4190396\ttotal: 7.02s\tremaining: 10.4s\n",
    "\n",
    "300:\tlearn: 0.3956015\ttotal: 10.4s\tremaining: 6.87s\n",
    "\n",
    "400:\tlearn: 0.3764075\ttotal: 13.7s\tremaining: 3.39s\n",
    "\n",
    "499:\tlearn: 0.3601961\ttotal: 17s\tremaining: 0us\n",
    "\n",
    "CatBoost Training Accuracy: **0.8604751583296644**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With SVD:\n",
    "0:\tlearn: 0.6564134\ttotal: 23.7ms\tremaining: 11.8s\n",
    "\n",
    "100:\tlearn: 0.4559546\ttotal: 1.94s\tremaining: 7.68s\n",
    "\n",
    "200:\tlearn: 0.4280927\ttotal: 3.91s\tremaining: 5.81s\n",
    "\n",
    "300:\tlearn: 0.4068786\ttotal: 5.88s\tremaining: 3.89s\n",
    "\n",
    "400:\tlearn: 0.3903511\ttotal: 7.88s\tremaining: 1.95s\n",
    "\n",
    "499:\tlearn: 0.3767146\ttotal: 9.87s\tremaining: 0us\n",
    "\n",
    "CatBoost Training Accuracy: **0.8494090154773597**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA for input features subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.read_csv(\"data/preprocessed_train.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/preprocessed_test.csv\")\n",
    "y_train = pd.read_csv(\"data/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_genetic_algorithm(X_data, y_data, population_size=30, n_generations=30, subset_size=30):\n",
    "    n_features = X_data.shape[1]\n",
    "    feature_names = X_data.columns.tolist()\n",
    "    \n",
    "    # Initialize population - each individual is a sorted list of feature indices\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        subset = random.sample(range(n_features), subset_size)\n",
    "        subset.sort()\n",
    "        population.append(subset)\n",
    "    \n",
    "    def fitness(individual):\n",
    "        X_subset = X_data.iloc[:, individual]\n",
    "        \n",
    "        # Manual train/test split instead of cross-validation\n",
    "        X_train_subset, X_val_subset, y_train_subset, y_val_subset = train_test_split(\n",
    "            X_subset, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            model = CatBoostClassifier(\n",
    "                iterations=100,  # Reduce from 500 to speed up GA\n",
    "                learning_rate=0.1,\n",
    "                depth=6,\n",
    "                class_weights=class_weights_dict,\n",
    "                random_seed=42,\n",
    "                verbose=0        # Turn off verbosity completely            \n",
    "            )\n",
    "            \n",
    "            # Use a simple fit instead of cross_val_score\n",
    "            model.fit(X_train_subset, y_train_subset, \n",
    "                     eval_set=(X_val_subset, y_val_subset),\n",
    "                     early_stopping_rounds=20,\n",
    "                     verbose=False)\n",
    "            \n",
    "            # Get validation accuracy\n",
    "            accuracy = accuracy_score(y_val_subset, model.predict(X_val_subset))\n",
    "            print(f\"Feature subset evaluated: accuracy = {accuracy:.4f}\")\n",
    "            return accuracy\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fitness evaluation: {e}\")\n",
    "            return 0.0  # Return worst fitness on error\n",
    "    \n",
    "    # Creates a child by merging features from both parents and selecting a random subset\n",
    "    def crossover(p1, p2, subset_size):\n",
    "        combined = list(set(p1) | set(p2))  # Union of features\n",
    "        if len(combined) > subset_size:\n",
    "            child = sorted(random.sample(combined, subset_size))  # Ensure correct size\n",
    "        else:\n",
    "            child = sorted(combined)  # Keep all if below subset_size\n",
    "        return child\n",
    "\n",
    "    # Mutation replaces a random index in child if random.threshold is met\n",
    "    def mutation(individual, n_features, subset_size):\n",
    "        if random.random() < 0.1:  # 10% chance of mutation\n",
    "            i = random.randrange(subset_size)\n",
    "            available_features = set(range(n_features)) - set(individual)  # Exclude existing features\n",
    "            if available_features:  \n",
    "                new_feature = random.choice(list(available_features))\n",
    "                individual[i] = new_feature\n",
    "                individual.sort()\n",
    "        return individual\n",
    "\n",
    "\n",
    "    for i in range(n_generations):\n",
    "        print(f\"\\nGeneration {i+1}/{n_generations}\")\n",
    "        # Evaluate fitness of population\n",
    "        print(\"Evaluating fitness for each individual:\")\n",
    "        scored_population = []\n",
    "        for idx, ind in enumerate(population):\n",
    "            fitness_score = fitness(ind)\n",
    "            scored_population.append((fitness_score, ind))\n",
    "            print(f\"Individual {idx+1}/{len(population)}: Fitness = {fitness_score:.4f}\")\n",
    "        \n",
    "        scored_population.sort(key=lambda x: x[0], reverse=True)\n",
    "        best_subset = scored_population[0][1]\n",
    "        print(f\"\\nBest fitness in generation {i+1}: {scored_population[0][0]:.4f}\")\n",
    "        print(\"Current best features:\", [feature_names[i] for i in best_subset])\n",
    "        \n",
    "        # Selection: truncation selection (pick top half as survivors)\n",
    "        survivors = scored_population[: population_size // 2]\n",
    "        \n",
    "        # Then randomly select two parents (p1 & p2) from survivors for crossover + mutation\n",
    "        print(\"Creating new population...\")\n",
    "        new_pop = [s[1] for s in survivors]\n",
    "        while len(new_pop) < population_size:\n",
    "            print(\"Generating new individual...\")\n",
    "            \n",
    "            p1 = random.choice(survivors)[1]\n",
    "            p2 = random.choice(survivors)[1]\n",
    "            child = crossover(p1, p2, subset_size)\n",
    "            child = mutation(child, n_features, subset_size)\n",
    "            \n",
    "            child = list(set(child))  # remove duplicates if any\n",
    "            while len(child) < subset_size:  # if duplicates reduced size\n",
    "                child.append(random.randrange(n_features))\n",
    "            child.sort()\n",
    "            new_pop.append(child)\n",
    "            \n",
    "            print(\"New individual created! Happy birthday!\")\n",
    "        population = new_pop\n",
    "        print(\"Current best:\", max([(fitness(ind), ind) for ind in population], key=lambda x: x[0])[1])\n",
    "    \n",
    "    best = max([(fitness(ind), ind) for ind in population], key=lambda x: x[0])[1]\n",
    "    print(\"\\nFinal Selected Features:\")\n",
    "    for idx, feature_idx in enumerate(best, 1):\n",
    "        print(f\"{idx}. {feature_names[feature_idx]}\")\n",
    "    print(f\"\\nTotal features selected: {len(best)}\")\n",
    "    return best\n",
    "\n",
    "best_features = run_genetic_algorithm(X_train_scaled, y_train)\n",
    "X_train_ga = X_train_scaled.iloc[:, best_features]\n",
    "X_test_ga = X_test_scaled.iloc[:, best_features]\n",
    "model = CatBoostClassifier(\n",
    "            iterations=500,\n",
    "            learning_rate=0.1,\n",
    "            depth=6,\n",
    "            class_weights=class_weights_dict,\n",
    "            random_seed=42,\n",
    "            verbose=100\n",
    "        )\n",
    "model.fit(X_train_ga, y_train)\n",
    "y_pred = model.predict(X_test_ga)\n",
    "print(\"Final features:\", best_features)\n",
    "print(\"Model accuracy with selected features:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total features selected: 30\n",
    "0:\tlearn: 0.6638991\ttotal: 24.9ms\tremaining: 12.4s\n",
    "100:\tlearn: 0.4927042\ttotal: 2.67s\tremaining: 10.5s\n",
    "200:\tlearn: 0.4564309\ttotal: 5.4s\tremaining: 8.04s\n",
    "300:\tlearn: 0.4329192\ttotal: 8.12s\tremaining: 5.37s\n",
    "400:\tlearn: 0.4150281\ttotal: 10.8s\tremaining: 2.67s\n",
    "499:\tlearn: 0.3979050\ttotal: 13.6s\tremaining: 0us\n",
    "Final features: [16, 36, 43, 63, 71, 99, 104, 138, 153, 161, 169, 171, 175, 182, 186, 191, 251, 270, 275, 281, 314, 325, 328, 348, 355, 358, 397, 400, 419, 428]\n",
    "Model accuracy with selected features: 0.8880177464693332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GA selected feature indices\n",
    "best_features = [16, 36, 43, 63, 71, 99, 104, 138, 153, 161, 169, 171, 175, 182, 186, 191, 251, 270, 275, 281, 314, 325, 328, 348, 355, 358, 397, 400, 419, 428]\n",
    "\n",
    "# Extract data with only selected features from GA\n",
    "X_train_ga = X_train_scaled.iloc[:, best_features]\n",
    "X_test_ga = X_test_scaled.iloc[:, best_features]\n",
    "\n",
    "print(f\"Original feature count: {X_train_scaled.shape[1]}\")\n",
    "print(f\"GA-selected feature count: {X_train_ga.shape[1]}\")\n",
    "print(f\"Training data shape after GA selection: {X_train_ga.shape}\")\n",
    "print(f\"Testing data shape after GA selection: {X_test_ga.shape}\")\n",
    "\n",
    "# Display feature names selected by GA\n",
    "print(\"\\nGA-selected features:\")\n",
    "feature_names = X_train_scaled.columns\n",
    "selected_feature_names = [feature_names[i] for i in best_features]\n",
    "for i, feature in enumerate(selected_feature_names, 1):\n",
    "    print(f\"{i}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train to numpy array (since it's read as DataFrame)\n",
    "y_train_array = y_train['isFraud'].values\n",
    "\n",
    "# Get unique classes and compute weights\n",
    "unique_classes = np.unique(y_train_array)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=unique_classes,\n",
    "    y=y_train_array\n",
    ")\n",
    "\n",
    "# Create dictionary of class weights\n",
    "class_weights_dict = dict(zip(unique_classes, class_weights))\n",
    "class_weights_dict = {int(k): float(v) for k, v in class_weights_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO and ACO for catboost hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyswarm import pso\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def run_pso_for_catboost_hyperparams(X_train_ga, y_train, X_test_ga, y_test, n_particles=10, n_iterations=5):\n",
    "    \"\"\"\n",
    "    Runs Particle Swarm Optimization (PSO) to find optimal hyperparameters for CatBoost using the provided training and testing data (already filtered by GA features).\n",
    "\n",
    "    Args:\n",
    "        X_train_ga: Training features (pandas DataFrame or numpy array) selected by GA.\n",
    "        y_train: Training target variable.\n",
    "        X_test_ga: Testing features (pandas DataFrame or numpy array) selected by GA.\n",
    "        y_test: Testing target variable.\n",
    "        n_particles: Number of particles in the swarm.\n",
    "        n_iterations: Number of iterations for PSO.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_params_dict, best_score)\n",
    "               - best_params_dict: Dictionary containing the best hyperparameter set found.\n",
    "               - best_score: The best AUC score achieved by PSO.\n",
    "    \"\"\"\n",
    "    print(f\"Starting PSO with {n_particles} particles and {n_iterations} iterations...\")\n",
    "\n",
    "    # Mapping for categorical parameters (indices):\n",
    "    grow_policy_map = ['SymmetricTree', 'Depthwise', 'Lossguide']\n",
    "    bootstrap_type_map = ['Bayesian', 'Bernoulli', 'MVS']\n",
    "    leaf_estimation_method_map = ['Newton', 'Gradient']\n",
    "    boosting_type_map = ['Ordered', 'Plain']\n",
    "\n",
    "    param_bounds_pso = {\n",
    "        # Key order must match the order in objective_function_pso's params array\n",
    "        'learning_rate': (0.01, 0.3),             # params[0]\n",
    "        'depth': (3, 10),                         # params[1] Integer\n",
    "        'l2_leaf_reg': (1.0, 10.0),               # params[2]\n",
    "        'iterations': (100, 1000),                # params[3] Integer\n",
    "        'grow_policy_idx': (0, len(grow_policy_map) - 1), # params[4] Integer index\n",
    "        'max_leaves': (4, 64),                    # params[5] Integer\n",
    "        'min_data_in_leaf': (1, 50),              # params[6] Integer\n",
    "        'random_strength': (0.1, 5.0),            # params[7]\n",
    "        'bagging_temperature': (0.0, 1.0),        # params[8]\n",
    "        'bootstrap_type_idx': (0, len(bootstrap_type_map) - 1), # params[9] Integer index\n",
    "        'subsample': (0.5, 1.0),                  # params[10] Used only if bootstrap_type is not Bayesian\n",
    "        'rsm': (0.5, 1.0),                        # params[11] aka colsample_bylevel\n",
    "        'leaf_estimation_method_idx': (0, len(leaf_estimation_method_map) - 1), # params[12] Integer index\n",
    "        'leaf_estimation_iterations': (1, 10),    # params[13] Integer\n",
    "        'boosting_type_idx': (0, len(boosting_type_map) - 1), # params[14] Integer index\n",
    "        'one_hot_max_size': (2, 30),              # params[15] Integer\n",
    "        'early_stopping_rounds': (10, 50)         # params[16] Integer, used in fit\n",
    "    }\n",
    "\n",
    "    # Ordered list of keys corresponding to the parameter order\n",
    "    param_keys = list(param_bounds_pso.keys())\n",
    "\n",
    "    # Define the objective function for PSO\n",
    "    def objective_function_pso(params):\n",
    "        # --- Parameter Extraction and Mapping ---\n",
    "        depth = int(round(params[param_keys.index('depth')]))\n",
    "        iterations = int(round(params[param_keys.index('iterations')]))\n",
    "        grow_policy_idx = int(round(params[param_keys.index('grow_policy_idx')]))\n",
    "        max_leaves = int(round(params[param_keys.index('max_leaves')]))\n",
    "        min_data_in_leaf = int(round(params[param_keys.index('min_data_in_leaf')]))\n",
    "        bootstrap_type_idx = int(round(params[param_keys.index('bootstrap_type_idx')]))\n",
    "        leaf_estimation_method_idx = int(round(params[param_keys.index('leaf_estimation_method_idx')]))\n",
    "        leaf_estimation_iterations = int(round(params[param_keys.index('leaf_estimation_iterations')]))\n",
    "        boosting_type_idx = int(round(params[param_keys.index('boosting_type_idx')]))\n",
    "        one_hot_max_size = int(round(params[param_keys.index('one_hot_max_size')]))\n",
    "        early_stopping_rounds = int(round(params[param_keys.index('early_stopping_rounds')]))\n",
    "\n",
    "        grow_policy = grow_policy_map[grow_policy_idx]\n",
    "        bootstrap_type = bootstrap_type_map[bootstrap_type_idx]\n",
    "        leaf_estimation_method = leaf_estimation_method_map[leaf_estimation_method_idx]\n",
    "        boosting_type = boosting_type_map[boosting_type_idx]\n",
    "\n",
    "        # --- Build CatBoost Params ---\n",
    "        cb_params = {\n",
    "            'learning_rate': params[param_keys.index('learning_rate')],\n",
    "            'depth': depth,\n",
    "            'l2_leaf_reg': params[param_keys.index('l2_leaf_reg')],\n",
    "            'iterations': iterations,\n",
    "            'grow_policy': grow_policy,\n",
    "            'max_leaves': max_leaves,\n",
    "            'min_data_in_leaf': min_data_in_leaf,\n",
    "            'random_strength': params[param_keys.index('random_strength')],\n",
    "            'bootstrap_type': bootstrap_type,\n",
    "            'rsm': params[param_keys.index('rsm')], # colsample_bylevel\n",
    "            'leaf_estimation_method': leaf_estimation_method,\n",
    "            'leaf_estimation_iterations': leaf_estimation_iterations,\n",
    "            'boosting_type': boosting_type,\n",
    "            'one_hot_max_size': one_hot_max_size,\n",
    "            'loss_function': 'Logloss',\n",
    "            'eval_metric': 'AUC',\n",
    "            'verbose': 0,\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        # Handle parameter dependencies\n",
    "        if bootstrap_type == 'Bayesian':\n",
    "            cb_params['bagging_temperature'] = params[param_keys.index('bagging_temperature')]\n",
    "        elif bootstrap_type in ['Bernoulli', 'MVS']:\n",
    "            cb_params['subsample'] = params[param_keys.index('subsample')]\n",
    "\n",
    "        # --- Optional: Create Validation Set ---\n",
    "        # If you want to use early stopping on a validation set instead of the test set:\n",
    "        # X_train_part, X_val_part, y_train_part, y_val_part = train_test_split(\n",
    "        #     X_train_ga, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        # )\n",
    "        # eval_set_data = (X_val_part, y_val_part)\n",
    "\n",
    "        # --- Train and Evaluate ---\n",
    "        model = CatBoostClassifier(**cb_params)\n",
    "\n",
    "        try:\n",
    "            # Using X_test_ga as eval set for early stopping\n",
    "            model.fit(X_train_ga, y_train,\n",
    "                      eval_set=(X_test_ga, y_test), # Or use eval_set_data if using validation split\n",
    "                      early_stopping_rounds=early_stopping_rounds,\n",
    "                      verbose=0)\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_test_ga)[:, 1]\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Exception during model training/evaluation: {e}\")\n",
    "            print(f\"Params causing issue: {cb_params}\")\n",
    "            # Return a very low score for problematic parameter combinations\n",
    "            auc = 0.0\n",
    "\n",
    "        # PSO maximizes, so return AUC directly\n",
    "        return auc\n",
    "\n",
    "    # Extract bounds for pyswarm\n",
    "    lb = [param_bounds_pso[k][0] for k in param_keys]\n",
    "    ub = [param_bounds_pso[k][1] for k in param_keys]\n",
    "\n",
    "    # Run PSO\n",
    "    best_params_pso_vals, best_score_pso = pso(objective_function_pso, lb, ub, swarmsize=n_particles, maxiter=n_iterations)\n",
    "\n",
    "    # --- Process Results ---\n",
    "    # Map best_params_pso_vals back to dictionary\n",
    "    best_params_pso_dict = {}\n",
    "    for i, key in enumerate(param_keys):\n",
    "        val = best_params_pso_vals[i]\n",
    "        if key in ['depth', 'iterations', 'grow_policy_idx', 'max_leaves', 'min_data_in_leaf',\n",
    "                   'bootstrap_type_idx', 'leaf_estimation_method_idx', 'leaf_estimation_iterations',\n",
    "                   'boosting_type_idx', 'one_hot_max_size', 'early_stopping_rounds']:\n",
    "            best_params_pso_dict[key] = int(round(val))\n",
    "        else:\n",
    "            best_params_pso_dict[key] = val\n",
    "\n",
    "    # Map indices back for categorical features\n",
    "    best_params_pso_dict['grow_policy'] = grow_policy_map[best_params_pso_dict['grow_policy_idx']]\n",
    "    best_params_pso_dict['bootstrap_type'] = bootstrap_type_map[best_params_pso_dict['bootstrap_type_idx']]\n",
    "    best_params_pso_dict['leaf_estimation_method'] = leaf_estimation_method_map[best_params_pso_dict['leaf_estimation_method_idx']]\n",
    "    best_params_pso_dict['boosting_type'] = boosting_type_map[best_params_pso_dict['boosting_type_idx']]\n",
    "\n",
    "    # Store the early stopping rounds value separately as it's used in fit, not constructor\n",
    "    best_early_stopping_rounds = best_params_pso_dict['early_stopping_rounds']\n",
    "\n",
    "    # Remove index keys and early stopping rounds from the main dict\n",
    "    keys_to_remove = ['grow_policy_idx', 'bootstrap_type_idx', 'leaf_estimation_method_idx', 'boosting_type_idx', 'early_stopping_rounds']\n",
    "    for key in keys_to_remove:\n",
    "       best_params_pso_dict.pop(key, None)\n",
    "\n",
    "    # Handle conditional subsample: Remove if not needed, ensure exists if needed\n",
    "    if best_params_pso_dict['bootstrap_type'] not in ['Bernoulli', 'MVS']:\n",
    "        best_params_pso_dict.pop('subsample', None)\n",
    "    elif 'subsample' not in best_params_pso_dict:\n",
    "         # If needed but missing (e.g., boundary issue), assign a default or average\n",
    "         best_params_pso_dict['subsample'] = np.mean(param_bounds_pso['subsample'])\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(\"PSO Finished.\")\n",
    "    print(f\"Best AUC score found by PSO: {best_score_pso:.6f}\")\n",
    "    print(\"Best CatBoost Parameters (excluding early_stopping_rounds):\")\n",
    "    for key, val in best_params_pso_dict.items():\n",
    "        print(f\"  {key}: {val}\")\n",
    "    print(f\"Best early_stopping_rounds: {best_early_stopping_rounds}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Add early stopping rounds back for potential direct use later if needed\n",
    "    best_params_pso_dict_final = best_params_pso_dict.copy()\n",
    "    best_params_pso_dict_final['early_stopping_rounds'] = best_early_stopping_rounds\n",
    "\n",
    "    return best_params_pso_dict_final, best_score_pso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split # Optional\n",
    "\n",
    "class ACO_HyperparameterOptimizer:\n",
    "    \"\"\"\n",
    "    Ant Colony Optimization for Hyperparameter Tuning.\n",
    "\n",
    "    Attributes:\n",
    "        param_grid (dict): Dictionary where keys are hyperparameter names\n",
    "                           and values are lists of discrete values to explore.\n",
    "        objective_function (callable): A function that takes a dictionary of\n",
    "                                      hyperparameters and returns a score to maximize.\n",
    "        n_ants (int): Number of ants (solutions generated per iteration).\n",
    "        n_iterations (int): Number of optimization iterations.\n",
    "        alpha (float): Pheromone influence factor.\n",
    "        beta (float): Heuristic influence factor (currently unused, set to 1).\n",
    "        rho (float): Pheromone evaporation rate (0 < rho <= 1).\n",
    "        Q (float): Pheromone deposit constant.\n",
    "        min_pheromone (float): Minimum pheromone level to prevent stagnation.\n",
    "        pheromones (dict): Nested dictionary storing pheromone levels for each parameter choice.\n",
    "        global_best_score (float): Best score found across all iterations.\n",
    "        global_best_params (dict): Hyperparameter set corresponding to the best score.\n",
    "    \"\"\"\n",
    "    def __init__(self, param_grid, objective_function, n_ants, n_iterations,\n",
    "                 alpha=1.0, beta=1.0, rho=0.1, Q=1.0, min_pheromone=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the ACO optimizer.\n",
    "        \"\"\"\n",
    "        self.param_grid = param_grid\n",
    "        self.objective_function = objective_function\n",
    "        self.n_ants = n_ants\n",
    "        self.n_iterations = n_iterations\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta  # Heuristic factor (often kept simple for HP tuning)\n",
    "        self.rho = rho    # Evaporation rate\n",
    "        self.Q = Q        # Pheromone deposit constant\n",
    "        self.min_pheromone = min_pheromone # Minimum pheromone level\n",
    "\n",
    "        self.param_names = list(self.param_grid.keys())\n",
    "        self.pheromones = self._initialize_pheromones()\n",
    "\n",
    "        self.global_best_score = -np.inf\n",
    "        self.global_best_params = None\n",
    "\n",
    "    def _initialize_pheromones(self):\n",
    "        \"\"\"\n",
    "        Initializes pheromone trails with a constant value for all parameter choices.\n",
    "        \"\"\"\n",
    "        pheromones = {}\n",
    "        initial_pheromone = 1.0 # Start with a neutral pheromone level\n",
    "        for param, values in self.param_grid.items():\n",
    "            pheromones[param] = {value: initial_pheromone for value in values}\n",
    "        return pheromones\n",
    "\n",
    "    def _select_next_node(self, param_name):\n",
    "        \"\"\"\n",
    "        Selects a value for a given parameter based on pheromone levels.\n",
    "        Uses a probabilistic choice mechanism influenced by pheromones.\n",
    "        \"\"\"\n",
    "        pheromone_values = self.pheromones[param_name]\n",
    "        choices = list(pheromone_values.keys())\n",
    "        current_pheromones = np.array([pheromone_values[choice] for choice in choices])\n",
    "\n",
    "        # Heuristic information (tau^alpha * eta^beta)\n",
    "        # For hyperparameter tuning, heuristic info (eta) is often uniform (beta=0 or eta=1)\n",
    "        # unless prior knowledge exists. We'll use beta=1 but eta=1 implicitly.\n",
    "        selection_probs = current_pheromones ** self.alpha # * (heuristic ** self.beta)\n",
    "\n",
    "        # Handle potential division by zero if all probs are zero (e.g., early stage)\n",
    "        prob_sum = np.sum(selection_probs)\n",
    "        if prob_sum == 0:\n",
    "            # If sum is zero, choose uniformly\n",
    "            selection_probs = np.ones(len(choices)) / len(choices)\n",
    "        else:\n",
    "            selection_probs = selection_probs / prob_sum\n",
    "\n",
    "        # Choose a value based on calculated probabilities\n",
    "        chosen_value = np.random.choice(choices, p=selection_probs)\n",
    "        return chosen_value\n",
    "\n",
    "    def _construct_solution(self):\n",
    "        \"\"\"\n",
    "        Constructs a complete hyperparameter set (solution) for a single ant.\n",
    "        \"\"\"\n",
    "        solution = {}\n",
    "        for param_name in self.param_names:\n",
    "            solution[param_name] = self._select_next_node(param_name)\n",
    "        return solution\n",
    "\n",
    "    def _update_pheromones(self, ant_solutions, ant_scores):\n",
    "        \"\"\"\n",
    "        Updates pheromone levels based on evaporation and deposition from ant solutions.\n",
    "        \"\"\"\n",
    "        # 1. Evaporation\n",
    "        for param, values in self.pheromones.items():\n",
    "            for value in values:\n",
    "                self.pheromones[param][value] *= (1.0 - self.rho)\n",
    "                # Enforce minimum pheromone level\n",
    "                self.pheromones[param][value] = max(self.pheromones[param][value], self.min_pheromone)\n",
    "\n",
    "        # 2. Deposition\n",
    "        # Deposit pheromone based on the quality of solutions found\n",
    "        # More sophisticated strategies exist (e.g., only best ant deposits, elitism)\n",
    "        # Here, all ants deposit based on their score\n",
    "        for i in range(self.n_ants):\n",
    "            solution = ant_solutions[i]\n",
    "            score = ant_scores[i]\n",
    "\n",
    "            # Normalize score or use directly? Depends on score range.\n",
    "            # Assuming score is something like AUC (0 to 1), direct use might be okay.\n",
    "            # We use Q as a scaling factor.\n",
    "            if score > -np.inf: # Only deposit if the evaluation was successful\n",
    "                delta_pheromone = self.Q * score # Simple proportional deposit\n",
    "\n",
    "                for param_name, value in solution.items():\n",
    "                    # Check if the parameter/value exists (it should)\n",
    "                    if param_name in self.pheromones and value in self.pheromones[param_name]:\n",
    "                        self.pheromones[param_name][value] += delta_pheromone\n",
    "                    else:\n",
    "                        # This case should ideally not happen if grid is consistent\n",
    "                         print(f\"Warning: Parameter '{param_name}' or value '{value}' not found in pheromone trails during deposition.\")\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Executes the ACO optimization process.\n",
    "        \"\"\"\n",
    "        print(f\"Starting ACO: {self.n_iterations} iterations, {self.n_ants} ants/iteration.\")\n",
    "        print(f\"Params: alpha={self.alpha}, beta={self.beta}, rho={self.rho}, Q={self.Q}\")\n",
    "\n",
    "        for iteration in range(self.n_iterations):\n",
    "            ant_solutions = []\n",
    "            ant_scores = []\n",
    "\n",
    "            # Generate solutions for all ants\n",
    "            for ant in range(self.n_ants):\n",
    "                solution = self._construct_solution()\n",
    "                try:\n",
    "                    score = self.objective_function(solution.copy()) # Pass a copy\n",
    "                    if score is None or not np.isfinite(score):\n",
    "                       # Handle cases where objective function fails or returns invalid score\n",
    "                       print(f\"Warning: Objective function returned invalid score ({score}) for params: {solution}. Assigning low score.\")\n",
    "                       score = -np.inf\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating solution: {solution}\")\n",
    "                    print(f\"Exception: {e}\")\n",
    "                    score = -np.inf # Penalize errors heavily\n",
    "\n",
    "                ant_solutions.append(solution)\n",
    "                ant_scores.append(score)\n",
    "\n",
    "                # Update global best if current ant is better\n",
    "                if score > self.global_best_score:\n",
    "                    self.global_best_score = score\n",
    "                    self.global_best_params = solution\n",
    "                    print(f\"Iteration {iteration+1}, Ant {ant+1}: New best score! -> {self.global_best_score:.6f}\")\n",
    "\n",
    "            # Update pheromone trails based on the iteration's results\n",
    "            self._update_pheromones(ant_solutions, ant_scores)\n",
    "\n",
    "            avg_score = np.mean([s for s in ant_scores if s > -np.inf]) if any(s > -np.inf for s in ant_scores) else -np.inf\n",
    "            print(f\"Iteration {iteration+1}/{self.n_iterations} finished. Avg Score: {avg_score:.6f}. Best Score so far: {self.global_best_score:.6f}\")\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "        print(\"ACO Finished.\")\n",
    "        if self.global_best_params:\n",
    "            print(f\"Best score found: {self.global_best_score:.6f}\")\n",
    "            print(\"Best parameters found:\")\n",
    "            for param, value in self.global_best_params.items():\n",
    "                print(f\"  {param}: {value}\")\n",
    "        else:\n",
    "            print(\"ACO did not find a valid solution.\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        return self.global_best_params, self.global_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_aco_for_catboost_hyperparams(X_train_ga, y_train, X_test_ga, y_test,\n",
    "                                     n_ants=10, n_iterations=5,\n",
    "                                     alpha=1.0, beta=1.0, rho=0.1, Q=1.0): # Add ACO params\n",
    "    \"\"\"\n",
    "    Runs Ant Colony Optimization (ACO) using the implemented class to find\n",
    "    optimal hyperparameters for CatBoost using the provided GA-filtered data.\n",
    "\n",
    "    Args:\n",
    "        X_train_ga: Training features (pandas DataFrame or numpy array) selected by GA.\n",
    "        y_train: Training target variable.\n",
    "        X_test_ga: Testing features (pandas DataFrame or numpy array) selected by GA.\n",
    "        y_test: Testing target variable.\n",
    "        n_ants (int): Number of ants (solutions per iteration).\n",
    "        n_iterations (int): Number of iterations for ACO.\n",
    "        alpha (float): Pheromone influence factor for ACO.\n",
    "        beta (float): Heuristic influence factor for ACO (currently unused).\n",
    "        rho (float): Pheromone evaporation rate for ACO.\n",
    "        Q (float): Pheromone deposit constant for ACO.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_params_dict, best_score)\n",
    "               - best_params_dict: Dictionary containing the best hyperparameter set found.\n",
    "               - best_score: The best AUC score achieved by ACO.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Define Parameter Grid (same as before) ---\n",
    "    param_grid_aco = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.3],\n",
    "        'depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'l2_leaf_reg': [1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "        'iterations': [100, 200, 300, 500, 700, 1000],\n",
    "        'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide'],\n",
    "        'max_leaves': [8, 16, 32, 64],\n",
    "        'min_data_in_leaf': [1, 5, 10, 20, 50],\n",
    "        'random_strength': [0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "        'bagging_temperature': [0.0, 0.2, 0.5, 0.8, 1.0],\n",
    "        'bootstrap_type': ['Bayesian', 'Bernoulli', 'MVS'],\n",
    "        'subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # Used only if bootstrap_type is not Bayesian\n",
    "        'rsm': [0.6, 0.7, 0.8, 0.9, 1.0], # colsample_bylevel\n",
    "        'leaf_estimation_method': ['Newton', 'Gradient'],\n",
    "        'leaf_estimation_iterations': [1, 3, 5, 10],\n",
    "        'boosting_type': ['Ordered', 'Plain'],\n",
    "        'one_hot_max_size': [2, 10, 20, 30],\n",
    "        'early_stopping_rounds': [10, 20, 30, 50] # Used in fit\n",
    "    }\n",
    "\n",
    "    # --- Define Objective Function (same as before) ---\n",
    "    def objective_function_aco(params):\n",
    "        # params is a dictionary constructed by the ACO framework for one ant\n",
    "        cb_params = params.copy()\n",
    "        early_stopping_rounds = cb_params.pop('early_stopping_rounds', 30) # Default if missing\n",
    "\n",
    "        # Handle parameter dependencies\n",
    "        bootstrap_type = cb_params.get('bootstrap_type', 'Bayesian')\n",
    "        if bootstrap_type == 'Bayesian':\n",
    "            # Keep bagging_temperature only for Bayesian bootstrap\n",
    "            if 'bagging_temperature' not in cb_params:\n",
    "                cb_params['bagging_temperature'] = 0.5  # default value\n",
    "            if 'subsample' in cb_params:\n",
    "                cb_params.pop('subsample')\n",
    "        else:\n",
    "            # For Bernoulli and MVS, use subsample and remove bagging_temperature\n",
    "            if 'bagging_temperature' in cb_params:\n",
    "                cb_params.pop('bagging_temperature')\n",
    "            if 'subsample' not in cb_params:\n",
    "                cb_params['subsample'] = 0.8  # default value\n",
    "\n",
    "        cb_params.update({\n",
    "            'loss_function': 'Logloss',\n",
    "            'eval_metric': 'AUC',\n",
    "            'verbose': 0,\n",
    "            'random_state': 42\n",
    "        })\n",
    "\n",
    "        model = CatBoostClassifier(**cb_params)\n",
    "        try:\n",
    "            model.fit(X_train_ga, y_train,\n",
    "                    eval_set=(X_test_ga, y_test),\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                    verbose=0)\n",
    "            y_pred_proba = model.predict_proba(X_test_ga)[:, 1]\n",
    "            auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            if not np.isfinite(auc):\n",
    "                auc = 0.0\n",
    "        except Exception as e:\n",
    "            auc = 0.0\n",
    "\n",
    "        return auc\n",
    "\n",
    "    # --- Instantiate and Run ACO ---\n",
    "    aco_optimizer = ACO_HyperparameterOptimizer(\n",
    "        param_grid=param_grid_aco,\n",
    "        objective_function=objective_function_aco,\n",
    "        n_ants=n_ants,\n",
    "        n_iterations=n_iterations,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        rho=rho,\n",
    "        Q=Q\n",
    "    )\n",
    "\n",
    "    best_params_aco, best_score_aco = aco_optimizer.run() # Run the optimization\n",
    "\n",
    "    # --- Process and Return Results ---\n",
    "    # The run method already prints results. We just return them.\n",
    "    # Note: best_params_aco will include 'early_stopping_rounds' if found.\n",
    "    return best_params_aco, best_score_aco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PSO and ACO optimizations using GA-selected features\n",
    "\n",
    "Now we'll run both PSO and ACO optimization algorithms on the same feature set selected by GA. This ensures fair comparison between optimization methods while benefiting from the dimension reduction provided by GA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PSO optimization on GA-selected features\n",
    "print(\"Starting PSO optimization on GA-selected features...\")\n",
    "best_pso_params, best_pso_score = run_pso_for_catboost_hyperparams(\n",
    "    X_train_ga, y_train, X_test_ga, y_test,\n",
    "    n_particles=10,\n",
    "    n_iterations=20\n",
    ")\n",
    "\n",
    "# Run ACO optimization on the same GA-selected features\n",
    "print(\"\\nStarting ACO optimization on GA-selected features...\")\n",
    "best_aco_params, best_aco_score = run_aco_for_catboost_hyperparams(\n",
    "    X_train_ga, y_train, X_test_ga, y_test,\n",
    "    n_ants=10,\n",
    "    n_iterations=20,\n",
    "    alpha=1.0,\n",
    "    beta=1.0,\n",
    "    rho=0.1,\n",
    "    Q=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate final models with optimized parameters on GA-selected features\n",
    "\n",
    "Both PSO and ACO have determined optimal hyperparameters for CatBoost. We'll now train final models with these parameters using only the GA-selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with PSO parameters on GA-selected features\n",
    "final_esr_pso = best_pso_params.pop('early_stopping_rounds')\n",
    "final_model_pso = CatBoostClassifier(\n",
    "    **best_pso_params,\n",
    "    random_state=42,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC'\n",
    ")\n",
    "final_model_pso.fit(\n",
    "    X_train_ga, y_train,\n",
    "    eval_set=(X_test_ga, y_test),\n",
    "    early_stopping_rounds=final_esr_pso,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Train final model with ACO parameters on GA-selected features\n",
    "final_esr_aco = best_aco_params.pop('early_stopping_rounds')\n",
    "final_model_aco = CatBoostClassifier(\n",
    "    **best_aco_params,\n",
    "    random_state=42,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC'\n",
    ")\n",
    "final_model_aco.fit(\n",
    "    X_train_ga, y_train,\n",
    "    eval_set=(X_test_ga, y_test),\n",
    "    early_stopping_rounds=final_esr_aco,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete for both PSO and ACO optimized models using GA-selected features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance analysis for GA-selected features\n",
    "\n",
    "Let's analyze which of the GA-selected features are most important according to our optimized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importance for GA-selected features\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_ga.columns,\n",
    "    'pso_importance': final_model_pso.get_feature_importance(),\n",
    "    'aco_importance': final_model_aco.get_feature_importance()\n",
    "})\n",
    "\n",
    "# Sort by average importance\n",
    "feature_importance['avg_importance'] = (feature_importance['pso_importance'] + \n",
    "                                       feature_importance['aco_importance']) / 2\n",
    "feature_importance = feature_importance.sort_values('avg_importance', ascending=False)\n",
    "\n",
    "# Reshape for seaborn\n",
    "plot_data = pd.melt(\n",
    "    feature_importance, \n",
    "    id_vars=['feature'], \n",
    "    value_vars=['pso_importance', 'aco_importance'],\n",
    "    var_name='model', \n",
    "    value_name='importance'\n",
    ")\n",
    "plot_data['model'] = plot_data['model'].map({'pso_importance': 'PSO', 'aco_importance': 'ACO'})\n",
    "\n",
    "# Top 15 features\n",
    "top_features = feature_importance.head(15)['feature'].tolist()\n",
    "plot_data_top = plot_data[plot_data['feature'].isin(top_features)]\n",
    "\n",
    "# Plot\n",
    "sns.barplot(\n",
    "    data=plot_data_top, \n",
    "    x='importance', \n",
    "    y='feature', \n",
    "    hue='model',\n",
    "    palette=['#2ecc71', '#e74c3c']\n",
    ")\n",
    "plt.title('Top 15 Important Features (GA-selected)', fontsize=14)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.legend(title='')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the top features and their importance scores\n",
    "print(\"\\nTop 10 Most Important GA-Selected Features:\")\n",
    "print(feature_importance[['feature', 'pso_importance', 'aco_importance', 'avg_importance']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare PSO, ACO, and baseline results on GA-selected features\n",
    "\n",
    "Now we'll do a comprehensive comparison of all three approaches: baseline CatBoost, PSO-optimized CatBoost, and ACO-optimized CatBoost, all using the same GA-selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "# Evaluate all three models\n",
    "print(\"Evaluating models on GA-selected features:\")\n",
    "\n",
    "# Get predictions for all models\n",
    "y_pred_baseline = baseline_model_ga.predict(X_test_ga)\n",
    "y_pred_pso = final_model_pso.predict(X_test_ga)\n",
    "y_pred_aco = final_model_aco.predict(X_test_ga)\n",
    "\n",
    "y_pred_proba_baseline = baseline_model_ga.predict_proba(X_test_ga)[:, 1]\n",
    "y_pred_proba_pso = final_model_pso.predict_proba(X_test_ga)[:, 1]\n",
    "y_pred_proba_aco = final_model_aco.predict_proba(X_test_ga)[:, 1]\n",
    "\n",
    "# Calculate metrics for all models\n",
    "models = {\n",
    "    'Baseline': (y_pred_baseline, y_pred_proba_baseline),\n",
    "    'PSO': (y_pred_pso, y_pred_proba_pso),\n",
    "    'ACO': (y_pred_aco, y_pred_proba_aco)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, (y_pred, y_pred_proba) in models.items():\n",
    "    results[name] = {\n",
    "        'accuracy': metrics.accuracy_score(y_test, y_pred),\n",
    "        'precision': metrics.precision_score(y_test, y_pred),\n",
    "        'recall': metrics.recall_score(y_test, y_pred),\n",
    "        'f1': metrics.f1_score(y_test, y_pred),\n",
    "        'auc': metrics.roc_auc_score(y_test, y_pred_proba),\n",
    "        'avg_precision': metrics.average_precision_score(y_test, y_pred_proba)\n",
    "    }\n",
    "\n",
    "# Display results as a table\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "comparison_df = comparison_df.round(4)\n",
    "print(\"\\nPerformance Comparison on GA-Selected Features:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# ROC curves comparison\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, (_, y_pred_proba) in models.items():\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison (GA-Selected Features)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, (_, y_pred_proba) in models.items():\n",
    "    precision, recall, _ = metrics.precision_recall_curve(y_test, y_pred_proba)\n",
    "    avg_precision = metrics.average_precision_score(y_test, y_pred_proba)\n",
    "    plt.plot(recall, precision, lw=2, label=f'{name} (AP = {avg_precision:.4f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves (GA-Selected Features)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of improvements\n",
    "print(\"\\nImprovement Summary on GA-Selected Features:\")\n",
    "baseline_metrics = results['Baseline']\n",
    "for name in ['PSO', 'ACO']:\n",
    "    print(f\"\\n{name} vs Baseline:\")\n",
    "    for metric, value in results[name].items():\n",
    "        improvement = value - baseline_metrics[metric]\n",
    "        improvement_pct = (improvement / baseline_metrics[metric]) * 100\n",
    "        print(f\"  {metric}: {improvement:.4f} absolute improvement ({improvement_pct:.2f}% relative improvement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare GA-selected features for final model deployment\n",
    "\n",
    "Let's save the final optimized model and export the list of GA-selected features for future use. This ensures we can reproduce the exact same preprocessing pipeline when deploying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best model based on AUC score\n",
    "best_model_name = max(results, key=lambda x: results[x]['auc'])\n",
    "print(f\"The best model based on AUC is: {best_model_name}\")\n",
    "\n",
    "# Create a reference dictionary with GA feature indices and their names\n",
    "ga_feature_mapping = {\n",
    "    'indices': best_features,\n",
    "    'names': selected_feature_names\n",
    "}\n",
    "\n",
    "# Export the GA feature information\n",
    "import json\n",
    "with open('ga_selected_features.json', 'w') as f:\n",
    "    json.dump(ga_feature_mapping, f, indent=4)\n",
    "\n",
    "# Save the best model\n",
    "if best_model_name == 'PSO':\n",
    "    best_model = final_model_pso\n",
    "elif best_model_name == 'ACO':\n",
    "    best_model = final_model_aco\n",
    "else:\n",
    "    best_model = baseline_model_ga\n",
    "\n",
    "best_model.save_model('fraud_detection_best_model.cbm')\n",
    "\n",
    "print(f\"\\nFinal model saved as 'fraud_detection_best_model.cbm'\")\n",
    "print(f\"GA-selected features saved to 'ga_selected_features.json'\")\n",
    "\n",
    "# Function to preprocess and predict using the saved model\n",
    "def predict_fraud(new_data, feature_mapping_path='ga_selected_features.json', model_path='fraud_detection_best_model.cbm'):\n",
    "    \"\"\"\n",
    "    Predict fraud using the saved model and GA-selected features\n",
    "    \n",
    "    Args:\n",
    "        new_data: DataFrame with the same columns as the original training data\n",
    "        feature_mapping_path: Path to the saved GA feature mapping\n",
    "        model_path: Path to the saved model\n",
    "        \n",
    "    Returns:\n",
    "        Fraud predictions (1 for fraud, 0 for not fraud)\n",
    "    \"\"\"\n",
    "    # Load feature mapping\n",
    "    with open(feature_mapping_path, 'r') as f:\n",
    "        feature_mapping = json.load(f)\n",
    "    \n",
    "    feature_names = feature_mapping['names']\n",
    "    \n",
    "    # Select only the GA-selected features\n",
    "    if all(name in new_data.columns for name in feature_names):\n",
    "        X_new = new_data[feature_names]\n",
    "    else:\n",
    "        indices = feature_mapping['indices']\n",
    "        X_new = new_data.iloc[:, indices]\n",
    "    \n",
    "    # Load model and predict\n",
    "    model = CatBoostClassifier()\n",
    "    model.load_model(model_path)\n",
    "    \n",
    "    return model.predict(X_new)\n",
    "\n",
    "print(\"\\nCreated utility function 'predict_fraud()' for easy model deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
