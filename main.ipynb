{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.stats as stats\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first and foremost, merge two dbs into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transaction = pd.read_csv(\"data/train_transaction.csv\")\n",
    "train_identity = pd.read_csv(\"data/train_identity.csv\")\n",
    "\n",
    "# Merge both dataframes on 'TransactionID'\n",
    "train = pd.merge(train_transaction, train_identity, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "print(f\"Rows in merged training set: {train.shape[0]}\")\n",
    "print(f\"Columns in merged training set: {train.shape[1]}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform an initial exploratory data analysis (EDA) by checking missing value percentages and examining the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identifying the target variable distribution and missing values for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Missing value percentages per column\n",
    "missing_percent = (train.isnull().sum() / len(train)) * 100\n",
    "missing_percent = missing_percent.sort_values(ascending=False)\n",
    "print(\"Missing percentages per column:\")\n",
    "print(missing_percent[missing_percent > 0])\n",
    "\n",
    "# Distribution of the target variable 'isFraud'\n",
    "train['isFraud'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Distribution of Fraudulent vs Non-Fraudulent Transactions\")\n",
    "plt.xlabel(\"isFraud\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_cols = [col for col in train.columns if train[col].isna().sum() > 0.9 * len(train)]\n",
    "null_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "missing_df = train.copy(deep=True)\n",
    "for col in null_cols:\n",
    "    missing_df[\"m_flag_\"+col] = np.where(missing_df[col].isnull(), 1, 0)\n",
    "    correlation = missing_df[[\"m_flag_\"+col, 'isFraud']].corr()\n",
    "    print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "categorical_features = train.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_features:\n",
    "    print(col, len(set(train[col])), set(train[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find truly categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "screen resolution values are true numerical values, while all other features are categorical in nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Screen_Width', 'Screen_Height']] = train['id_33'].str.split('x', expand=True).astype(float)\n",
    "train = train.drop(columns=['id_33'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cat_cols = train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Identify candidate categorical features based on unique value counts\n",
    "candidate_categorical = {}\n",
    "# Set a threshold for maximum unique values\n",
    "unique_threshold = 20\n",
    "\n",
    "# Iterate over numeric columns to check unique value counts\n",
    "for col in train.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    unique_vals = train[col].nunique()\n",
    "    if unique_vals < unique_threshold and col != \"isFraud\":\n",
    "        candidate_categorical[col] = unique_vals\n",
    "\n",
    "# Print candidate categorical features\n",
    "print(\"Candidate categorical features (numeric columns with few unique values):\")\n",
    "for col, count in candidate_categorical.items():\n",
    "    print(f\"{col}: {count} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not using standard imputation:\n",
    "1. placed zero values as indicator for missing values where feature values no zero values anywhere else\n",
    "2. added binary \"missing_...\" column for other features with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Identify numeric and categorical columns\n",
    "num_cols = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = list(set(cat_cols).union(set(candidate_categorical.keys())))\n",
    "num_cols = [col for col in num_cols if col not in cat_cols and col not in (\"TransactionID\", \"isFraud\")]\n",
    "\n",
    "# Imputation for numeric columns using median strategy; add indicator if desired\n",
    "num_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "train[num_cols] = num_imputer.fit_transform(train[num_cols])\n",
    "\n",
    "# Imputation for categorical columns using a constant value\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "train[cat_cols] = cat_imputer.fit_transform(train[cat_cols])\n",
    "\n",
    "# Confirm that no missing values remain (or check overall missing count)\n",
    "print(\"Total missing values after imputation:\", train.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=[\"isFraud\", \"TransactionID\"])\n",
    "y = train['isFraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting train and test asap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using novel thing: WOE encoder to compensate for an enormous dimentionality for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_high = ce.WOEEncoder(cols=cat_cols)\n",
    "X_train_encoded_cat = encoder_high.fit_transform(X_train[cat_cols], y_train)\n",
    "X_test_encoded_cat = encoder_high.transform(X_test[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data to remove any disrepancies in SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled_num = scaler.fit_transform(X_train[num_cols])\n",
    "X_test_scaled_num = scaler.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(np.hstack([X_train_encoded_cat.values, X_train_scaled_num]), columns=cat_cols + num_cols)\n",
    "X_test_scaled = pd.DataFrame(np.hstack([X_test_encoded_cat.values, X_test_scaled_num]), columns=cat_cols + num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(\"Encoded training set shape:\", X_train_scaled.shape)\n",
    "print(\"Encoded test set shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.to_csv(\"data/preprocessed_train.csv\", index=False)\n",
    "X_test_scaled.to_csv(\"data/preprocessed_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = SVC(kernel=\"rbf\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Baseline model Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA for input features subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_genetic_algorithm(X_data, y_data, population_size=30, n_generations=5, subset_size=20):\n",
    "    n_features = X_data.shape[1]\n",
    "    \n",
    "    # Initialize population - each individual is a sorted list of feature indices\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        subset = random.sample(range(n_features), subset_size)\n",
    "        subset.sort()\n",
    "        population.append(subset)\n",
    "    \n",
    "    def fitness(individual):\n",
    "        # Train SVC with this subset and return average cross-validation accuracy\n",
    "        X_subset = X_data.iloc[:, individual]\n",
    "        model = SVC(kernel=\"rbf\")\n",
    "        scores = cross_val_score(model, X_subset, y_data, cv=3, scoring='accuracy')\n",
    "        return scores.mean()\n",
    "    \n",
    "    # Crossover combines genes from both parents at a random cut point\n",
    "    def crossover(p1, p2, subset_size):\n",
    "        cut = random.randint(1, subset_size - 1)\n",
    "        return p1[:cut] + p2[cut:]\n",
    "\n",
    "    # Mutation replaces a random index in child if random.threshold is met\n",
    "    def mutation(child, n_features, subset_size):\n",
    "        if random.random() < 0.1:\n",
    "            i = random.randrange(subset_size)\n",
    "            child[i] = random.randrange(n_features)\n",
    "        return child\n",
    "\n",
    "    for _ in range(n_generations):\n",
    "        # Evaluate fitness of population\n",
    "        scored_population = [(fitness(ind), ind) for ind in population]\n",
    "        scored_population.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Selection: truncation selection (pick top half as survivors)\n",
    "        survivors = scored_population[: population_size // 2]\n",
    "        \n",
    "        # Then randomly select two parents (p1 & p2) from survivors for crossover + mutation\n",
    "        new_pop = [s[1] for s in survivors]\n",
    "        while len(new_pop) < population_size:\n",
    "            p1 = random.choice(survivors)[1]\n",
    "            p2 = random.choice(survivors)[1]\n",
    "            child = crossover(p1, p2, subset_size)\n",
    "            child = mutation(child, n_features, subset_size)\n",
    "            \n",
    "            child = list(set(child))  # remove duplicates if any\n",
    "            while len(child) < subset_size:  # if duplicates reduced size\n",
    "                child.append(random.randrange(n_features))\n",
    "            child.sort()\n",
    "            new_pop.append(child)\n",
    "        population = new_pop\n",
    "    \n",
    "    best = max([(fitness(ind), ind) for ind in population], key=lambda x: x[0])[1]\n",
    "    return best\n",
    "\n",
    "best_features = run_genetic_algorithm(X_train, y_train)\n",
    "X_train_ga = X_train.iloc[:, best_features]\n",
    "X_test_ga = X_test.iloc[:, best_features]\n",
    "model = SVC(kernel=\"rbf\")\n",
    "model.fit(X_train_ga, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Model accuracy with selected features:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO and ACO for SVC hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pso_for_hyperparams(X_data, y_data, model_class, n_particles=10, n_iterations=5):\n",
    "    # Define parameter search space for SVC (C and gamma)\n",
    "    c_min, c_max = 0.01, 10.0\n",
    "    gamma_min, gamma_max = 1e-4, 1.0\n",
    "\n",
    "    # Initialize particles (positions) and velocities randomly\n",
    "    positions = [\n",
    "        [random.uniform(c_min, c_max), random.uniform(gamma_min, gamma_max)]\n",
    "        for _ in range(n_particles)\n",
    "    ]\n",
    "    velocities = [[0.0, 0.0] for _ in range(n_particles)]\n",
    "\n",
    "    # Personal bests (pbest) and global best (gbest)\n",
    "    pbest_positions = positions[:]\n",
    "    pbest_scores = [0.0]*n_particles\n",
    "    gbest_position = None\n",
    "    gbest_score = 0.0\n",
    "\n",
    "    def fitness(pos):\n",
    "        candidate = model_class(C=pos[0], gamma=pos[1], kernel='rbf')\n",
    "        scores = cross_val_score(candidate, X_data, y_data, cv=3, scoring='accuracy')\n",
    "        return scores.mean()\n",
    "\n",
    "    # Evaluate initial fitness\n",
    "    for i in range(n_particles):\n",
    "        current_score = fitness(positions[i])\n",
    "        pbest_scores[i] = current_score\n",
    "        if current_score > gbest_score:\n",
    "            gbest_score = current_score\n",
    "            gbest_position = positions[i][:]\n",
    "\n",
    "    # Main PSO loop\n",
    "    w = 0.5  # inertia\n",
    "    c1 = 1.0 # cognitive\n",
    "    c2 = 1.0 # social\n",
    "    for _ in range(n_iterations):\n",
    "        for i in range(n_particles):\n",
    "            # Velocity update\n",
    "            r1, r2 = random.random(), random.random()\n",
    "            velocities[i][0] = (\n",
    "                w*velocities[i][0]\n",
    "                + c1*r1*(pbest_positions[i][0] - positions[i][0])\n",
    "                + c2*r2*(gbest_position[0] - positions[i][0])\n",
    "            )\n",
    "            velocities[i][1] = (\n",
    "                w*velocities[i][1]\n",
    "                + c1*r1*(pbest_positions[i][1] - positions[i][1])\n",
    "                + c2*r2*(gbest_position[1] - positions[i][1])\n",
    "            )\n",
    "            \n",
    "            # Position update and boundary check\n",
    "            positions[i][0] += velocities[i][0]\n",
    "            positions[i][1] += velocities[i][1]\n",
    "            positions[i][0] = max(c_min, min(c_max, positions[i][0]))\n",
    "            positions[i][1] = max(gamma_min, min(gamma_max, positions[i][1]))\n",
    "\n",
    "            # Evaluate fitness\n",
    "            current_score = fitness(positions[i])\n",
    "            if current_score > pbest_scores[i]:\n",
    "                pbest_scores[i] = current_score\n",
    "                pbest_positions[i] = positions[i][:]\n",
    "                if current_score > gbest_score:\n",
    "                    gbest_score = current_score\n",
    "                    gbest_position = positions[i][:]\n",
    "\n",
    "    return {\"C\": gbest_position[0], \"gamma\": gbest_position[1], \"kernel\": \"rbf\"}\n",
    "\n",
    "best_params = run_pso_for_hyperparams(X_train_ga, y_train, SVC)\n",
    "model_pso = SVC(**best_params)\n",
    "model_pso.fit(X_train_ga, y_train)\n",
    "y_pred_pso = model_pso.predict(X_test_ga)\n",
    "print(\"PSO-tuned model Accuracy:\", accuracy_score(y_test, y_pred_pso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_aco_for_hyperparams(X_data, y_data, model_class, n_ants=10, n_iterations=5):\n",
    "    # Define parameter search space for SVC (C and gamma)\n",
    "    c_min, c_max = 0.01, 10.0\n",
    "    gamma_min, gamma_max = 1e-4, 1.0\n",
    "\n",
    "    # Initialize pheromone levels for discrete “bins” in each dimension\n",
    "    n_bins = 10\n",
    "    pheromones = [[1.0 for _ in range(n_bins)] for _ in range(n_bins)]\n",
    "\n",
    "    def bin_to_value(bin_index, min_val, max_val):\n",
    "        step_size = (max_val - min_val) / n_bins\n",
    "        return min_val + bin_index * step_size + (step_size / 2)\n",
    "\n",
    "    def fitness(c_val, gamma_val):\n",
    "        candidate = model_class(C=c_val, gamma=gamma_val, kernel='rbf')\n",
    "        scores = cross_val_score(candidate, X_data, y_data, cv=3, scoring='accuracy')\n",
    "        return scores.mean()\n",
    "\n",
    "    best_score = 0.0\n",
    "    best_params = (1.0, 1e-3)\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        solutions = []\n",
    "        for _ant in range(n_ants):\n",
    "            # Select bins for C, gamma by roulette wheel sampling of pheromones\n",
    "            c_bin = random.choices(range(n_bins), weights=pheromones[0], k=1)[0]\n",
    "            g_bin = random.choices(range(n_bins), weights=pheromones[1], k=1)[0]\n",
    "            c_val = bin_to_value(c_bin, c_min, c_max)\n",
    "            gamma_val = bin_to_value(g_bin, gamma_min, gamma_max)\n",
    "            \n",
    "            score = fitness(c_val, gamma_val)\n",
    "            solutions.append((c_bin, g_bin, score))\n",
    "\n",
    "        # Update pheromones\n",
    "        # Evaporate\n",
    "        for i in range(n_bins):\n",
    "            for j in range(len(pheromones)):\n",
    "                pheromones[j][i] *= 0.9\n",
    "\n",
    "        # Deposit\n",
    "        for (c_bin, g_bin, score) in solutions:\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = (bin_to_value(c_bin, c_min, c_max),\n",
    "                               bin_to_value(g_bin, gamma_min, gamma_max))\n",
    "            pheromones[0][c_bin] += score\n",
    "            pheromones[1][g_bin] += score\n",
    "\n",
    "    return {\"C\": best_params[0], \"gamma\": best_params[1], \"kernel\": \"rbf\"}\n",
    "\n",
    "best_params_aco = run_aco_for_hyperparams(X_train_ga, y_train, SVC)\n",
    "model_aco = SVC(**best_params_aco)\n",
    "model_aco.fit(X_train_ga, y_train)\n",
    "y_pred_aco = model_aco.predict(X_test_ga)\n",
    "print(\"ACO-tuned model Accuracy:\", accuracy_score(y_test, y_pred_aco))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO and ACO for SVC loss function convergence speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hell no we're not doing this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
