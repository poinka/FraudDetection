{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Additional encoders\n",
    "import category_encoders as ce\n",
    "\n",
    "# Stats\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import (\n",
    "    PCA,\n",
    "    KernelPCA, \n",
    "    FastICA,\n",
    "    TruncatedSVD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First and foremost, merge two dbs into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction = pd.read_csv(\"data/train_transaction.csv\")\n",
    "train_identity = pd.read_csv(\"data/train_identity.csv\")\n",
    "\n",
    "# Merge both dataframes on 'TransactionID'\n",
    "train = pd.merge(train_transaction, train_identity, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "print(f\"Rows in merged training set: {train.shape[0]}\")\n",
    "print(f\"Columns in merged training set: {train.shape[1]}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform an initial exploratory data analysis (EDA) by checking missing value percentages and examining the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying the target variable distribution and missing values for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value percentages per column\n",
    "missing_percent = (train.isnull().sum() / len(train)) * 100\n",
    "missing_percent = missing_percent.sort_values(ascending=False)\n",
    "print(\"Missing percentages per column:\")\n",
    "print(missing_percent[missing_percent > 0])\n",
    "\n",
    "# Distribution of the target variable 'isFraud'\n",
    "train['isFraud'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Distribution of Fraudulent vs Non-Fraudulent Transactions\")\n",
    "plt.xlabel(\"isFraud\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cols = [col for col in train.columns if train[col].isna().sum() > 0.9 * len(train)]\n",
    "null_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = train.copy(deep=True)\n",
    "for col in null_cols:\n",
    "    missing_df[\"m_flag_\"+col] = np.where(missing_df[col].isnull(), 1, 0)\n",
    "    correlation = missing_df[[\"m_flag_\"+col, 'isFraud']].corr()\n",
    "    print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = train.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_features:\n",
    "    print(col, len(set(train[col])), set(train[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find truly categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Screen resolution values are true numerical values, while all other features are categorical in nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Screen_Width', 'Screen_Height']] = train['id_33'].str.split('x', expand=True).astype(float)\n",
    "train = train.drop(columns=['id_33'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Identify candidate categorical features based on unique value counts\n",
    "candidate_categorical = {}\n",
    "# Set a threshold for maximum unique values\n",
    "unique_threshold = 20\n",
    "\n",
    "# Iterate over numeric columns to check unique value counts\n",
    "for col in train.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    unique_vals = train[col].nunique()\n",
    "    if unique_vals < unique_threshold and col != \"isFraud\":\n",
    "        candidate_categorical[col] = unique_vals\n",
    "\n",
    "# Print candidate categorical features\n",
    "print(\"Candidate categorical features (numeric columns with few unique values):\")\n",
    "for col, count in candidate_categorical.items():\n",
    "    print(f\"{col}: {count} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not using standard imputation:\n",
    "1. Placed zero values as indicator for missing values where feature values no zero values anywhere else\n",
    "2. Added 'missing' instead of null for categorical values to keep all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical columns\n",
    "num_cols = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = list(set(cat_cols).union(set(candidate_categorical.keys())))\n",
    "num_cols = [col for col in num_cols if col not in cat_cols and col not in (\"TransactionID\", \"isFraud\")]\n",
    "\n",
    "# Imputation for numeric columns using zeros as indicator\n",
    "num_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "train[num_cols] = num_imputer.fit_transform(train[num_cols])\n",
    "\n",
    "# Imputation for categorical columns using a constant value\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "train[cat_cols] = cat_imputer.fit_transform(train[cat_cols])\n",
    "\n",
    "# Confirm that no missing values remain (or check overall missing count)\n",
    "print(\"Total missing values after imputation:\", train.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=[\"isFraud\", \"TransactionID\"])\n",
    "y = train['isFraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting train and test asap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using novel thing: WOE encoder to compensate for an enormous dimentionality for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_high = ce.WOEEncoder(cols=cat_cols)\n",
    "X_train_encoded_cat = encoder_high.fit_transform(X_train[cat_cols], y_train)\n",
    "X_test_encoded_cat = encoder_high.transform(X_test[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data to remove any disrepancies in SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled_num = scaler.fit_transform(X_train[num_cols])\n",
    "X_test_scaled_num = scaler.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(np.hstack([X_train_encoded_cat.values, X_train_scaled_num]), columns=cat_cols + num_cols)\n",
    "X_test_scaled = pd.DataFrame(np.hstack([X_test_encoded_cat.values, X_test_scaled_num]), columns=cat_cols + num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoded training set shape:\", X_train_scaled.shape)\n",
    "print(\"Encoded test set shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.to_csv(\"data/preprocessed_train.csv\", index=False)\n",
    "X_test_scaled.to_csv(\"data/preprocessed_test.csv\", index=False)\n",
    "\n",
    "y_train.to_csv(\"data/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"data/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.read_csv(\"data/preprocessed_train.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/preprocessed_test.csv\")\n",
    "y_train = pd.read_csv(\"data/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accounting for severe class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train to numpy array (since it's read as DataFrame)\n",
    "y_train_array = y_train['isFraud'].values\n",
    "\n",
    "# Get unique classes and compute weights\n",
    "unique_classes = np.unique(y_train_array)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=unique_classes,\n",
    "    y=y_train_array\n",
    ")\n",
    "\n",
    "# Create dictionary of class weights\n",
    "class_weights_dict = dict(zip(unique_classes, class_weights))\n",
    "class_weights_dict = {int(k): float(v) for k, v in class_weights_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30)\n",
    "X_train_scaled = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(n_components=30, kernel='rbf', random_state=42)\n",
    "X_train_scaled = kpca.fit_transform(X_train_scaled)\n",
    "X_test_scaled = kpca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent Component Analysis (ICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components=30, random_state=42)\n",
    "X_train_scaled = ica.fit_transform(X_train_scaled)\n",
    "X_test_scaled = ica.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=30, random_state=42)\n",
    "X_train_scaled = svd.fit_transform(X_train_scaled)\n",
    "X_test_scaled = svd.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6556789\ttotal: 114ms\tremaining: 56.9s\n",
      "100:\tlearn: 0.4566328\ttotal: 3.24s\tremaining: 12.8s\n",
      "200:\tlearn: 0.4297046\ttotal: 5.49s\tremaining: 8.17s\n",
      "300:\tlearn: 0.4090670\ttotal: 7.71s\tremaining: 5.1s\n",
      "400:\tlearn: 0.3936705\ttotal: 9.92s\tremaining: 2.45s\n",
      "499:\tlearn: 0.3800877\ttotal: 12.1s\tremaining: 0us\n",
      "CatBoost Accuracy: 0.8490026077827073\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "y_train_flat = y_train.values.ravel()\n",
    "y_test_flat = y_test.values.ravel()\n",
    "\n",
    "model_cat = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    class_weights=class_weights_dict,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "model_cat.fit(X_train_scaled, y_train_flat)\n",
    "y_pred_cat = model_cat.predict(X_test_scaled)\n",
    "print(\"CatBoost Training Accuracy:\", accuracy_score(y_test_flat, y_pred_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA for input features subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_genetic_algorithm(X_data, y_data, population_size=30, n_generations=5, subset_size=20):\n",
    "    n_features = X_data.shape[1]\n",
    "    \n",
    "    # Initialize population - each individual is a sorted list of feature indices\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        subset = random.sample(range(n_features), subset_size)\n",
    "        subset.sort()\n",
    "        population.append(subset)\n",
    "    \n",
    "    def fitness(individual):\n",
    "        # Train SVC with this subset and return average cross-validation accuracy\n",
    "        X_subset = X_data.iloc[:, individual]\n",
    "        model = SVC(kernel=\"rbf\")\n",
    "        scores = cross_val_score(model, X_subset, y_data, cv=3, scoring='accuracy')\n",
    "        return scores.mean()\n",
    "    \n",
    "    # Crossover combines genes from both parents at a random cut point\n",
    "    def crossover(p1, p2, subset_size):\n",
    "        cut = random.randint(1, subset_size - 1)\n",
    "        return p1[:cut] + p2[cut:]\n",
    "\n",
    "    # Mutation replaces a random index in child if random.threshold is met\n",
    "    def mutation(child, n_features, subset_size):\n",
    "        if random.random() < 0.1:\n",
    "            i = random.randrange(subset_size)\n",
    "            child[i] = random.randrange(n_features)\n",
    "        return child\n",
    "\n",
    "    for _ in range(n_generations):\n",
    "        # Evaluate fitness of population\n",
    "        scored_population = [(fitness(ind), ind) for ind in population]\n",
    "        scored_population.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Selection: truncation selection (pick top half as survivors)\n",
    "        survivors = scored_population[: population_size // 2]\n",
    "        \n",
    "        # Then randomly select two parents (p1 & p2) from survivors for crossover + mutation\n",
    "        new_pop = [s[1] for s in survivors]\n",
    "        while len(new_pop) < population_size:\n",
    "            p1 = random.choice(survivors)[1]\n",
    "            p2 = random.choice(survivors)[1]\n",
    "            child = crossover(p1, p2, subset_size)\n",
    "            child = mutation(child, n_features, subset_size)\n",
    "            \n",
    "            child = list(set(child))  # remove duplicates if any\n",
    "            while len(child) < subset_size:  # if duplicates reduced size\n",
    "                child.append(random.randrange(n_features))\n",
    "            child.sort()\n",
    "            new_pop.append(child)\n",
    "        population = new_pop\n",
    "    \n",
    "    best = max([(fitness(ind), ind) for ind in population], key=lambda x: x[0])[1]\n",
    "    return best\n",
    "\n",
    "best_features = run_genetic_algorithm(X_train, y_train)\n",
    "X_train_ga = X_train.iloc[:, best_features]\n",
    "X_test_ga = X_test.iloc[:, best_features]\n",
    "model = SVC(kernel=\"rbf\")\n",
    "model.fit(X_train_ga, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Model accuracy with selected features:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO and ACO for catboost hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pso_for_catboost_hyperparams(X_data, y_data, n_particles=10, n_iterations=5):\n",
    "    # Define parameter search spaces\n",
    "    param_bounds = {\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'depth': (3, 10),\n",
    "        'l2_leaf_reg': (1.0, 10.0),\n",
    "        'iterations': (100, 1000)\n",
    "    }\n",
    "    \n",
    "    # Initialize particles\n",
    "    positions = []\n",
    "    for _ in range(n_particles):\n",
    "        pos = {\n",
    "            'learning_rate': random.uniform(*param_bounds['learning_rate']),\n",
    "            'depth': int(random.uniform(*param_bounds['depth'])),\n",
    "            'l2_leaf_reg': random.uniform(*param_bounds['l2_leaf_reg']),\n",
    "            'iterations': int(random.uniform(*param_bounds['iterations']))\n",
    "        }\n",
    "        positions.append(pos)\n",
    "    \n",
    "    velocities = [{k: 0.0 for k in param_bounds.keys()} for _ in range(n_particles)]\n",
    "    \n",
    "    # Personal and global bests\n",
    "    pbest_positions = positions[:]\n",
    "    pbest_scores = [0.0] * n_particles\n",
    "    gbest_position = None\n",
    "    gbest_score = 0.0\n",
    "    \n",
    "    def fitness(params):\n",
    "        model = CatBoostClassifier(\n",
    "            **params,\n",
    "            class_weights=class_weights_dict,\n",
    "            random_seed=42,\n",
    "            verbose=False\n",
    "        )\n",
    "        scores = cross_val_score(model, X_data, y_data, cv=3, scoring='accuracy')\n",
    "        return scores.mean()\n",
    "    \n",
    "    # Evaluate initial fitness\n",
    "    for i in range(n_particles):\n",
    "        score = fitness(positions[i])\n",
    "        pbest_scores[i] = score\n",
    "        if score > gbest_score:\n",
    "            gbest_score = score\n",
    "            gbest_position = positions[i].copy()\n",
    "    \n",
    "    # Main PSO loop\n",
    "    w, c1, c2 = 0.5, 1.0, 1.0\n",
    "    for _ in range(n_iterations):\n",
    "        for i in range(n_particles):\n",
    "            r1, r2 = random.random(), random.random()\n",
    "            \n",
    "            # Update velocities and positions for each parameter\n",
    "            for param in param_bounds.keys():\n",
    "                velocities[i][param] = (\n",
    "                    w * velocities[i][param] +\n",
    "                    c1 * r1 * (pbest_positions[i][param] - positions[i][param]) +\n",
    "                    c2 * r2 * (gbest_position[param] - positions[i][param])\n",
    "                )\n",
    "                \n",
    "                positions[i][param] += velocities[i][param]\n",
    "                \n",
    "                # Ensure bounds and proper types\n",
    "                if param == 'depth' or param == 'iterations':\n",
    "                    positions[i][param] = int(max(param_bounds[param][0],\n",
    "                                               min(param_bounds[param][1],\n",
    "                                                   positions[i][param])))\n",
    "                else:\n",
    "                    positions[i][param] = max(param_bounds[param][0],\n",
    "                                           min(param_bounds[param][1],\n",
    "                                               positions[i][param]))\n",
    "            \n",
    "            # Evaluate fitness\n",
    "            score = fitness(positions[i])\n",
    "            if score > pbest_scores[i]:\n",
    "                pbest_scores[i] = score\n",
    "                pbest_positions[i] = positions[i].copy()\n",
    "                if score > gbest_score:\n",
    "                    gbest_score = score\n",
    "                    gbest_position = positions[i].copy()\n",
    "    \n",
    "    return gbest_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_aco_for_catboost_hyperparams(X_data, y_data, n_ants=10, n_iterations=5):\n",
    "    # Define parameter spaces and bins\n",
    "    n_bins = 10\n",
    "    param_bounds = {\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'depth': (3, 10),\n",
    "        'l2_leaf_reg': (1.0, 10.0),\n",
    "        'iterations': (100, 1000)\n",
    "    }\n",
    "    \n",
    "    # Initialize pheromone levels for each parameter\n",
    "    pheromones = {param: [1.0] * n_bins for param in param_bounds.keys()}\n",
    "    \n",
    "    def bin_to_value(bin_index, param):\n",
    "        min_val, max_val = param_bounds[param]\n",
    "        step_size = (max_val - min_val) / n_bins\n",
    "        val = min_val + bin_index * step_size + (step_size / 2)\n",
    "        return int(val) if param in ['depth', 'iterations'] else val\n",
    "    \n",
    "    def fitness(params):\n",
    "        model = CatBoostClassifier(\n",
    "            **params,\n",
    "            class_weights=class_weights_dict,\n",
    "            random_seed=42,\n",
    "            verbose=False\n",
    "        )\n",
    "        scores = cross_val_score(model, X_data, y_data, cv=3, scoring='accuracy')\n",
    "        return scores.mean()\n",
    "    \n",
    "    best_score = 0.0\n",
    "    best_params = None\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        solutions = []\n",
    "        \n",
    "        # Generate solutions for each ant\n",
    "        for _ in range(n_ants):\n",
    "            current_params = {}\n",
    "            for param in param_bounds.keys():\n",
    "                bin_idx = random.choices(range(n_bins), \n",
    "                                      weights=pheromones[param], \n",
    "                                      k=1)[0]\n",
    "                current_params[param] = bin_to_value(bin_idx, param)\n",
    "            \n",
    "            score = fitness(current_params)\n",
    "            solutions.append((current_params, score))\n",
    "        \n",
    "        # Evaporate pheromones\n",
    "        for param in pheromones:\n",
    "            for i in range(n_bins):\n",
    "                pheromones[param][i] *= 0.9\n",
    "        \n",
    "        # Update pheromones based on solutions\n",
    "        for params, score in solutions:\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params.copy()\n",
    "            \n",
    "            # Deposit pheromones\n",
    "            for param, value in params.items():\n",
    "                bin_idx = int((value - param_bounds[param][0]) / \n",
    "                            (param_bounds[param][1] - param_bounds[param][0]) * n_bins)\n",
    "                bin_idx = min(bin_idx, n_bins - 1)\n",
    "                pheromones[param][bin_idx] += score\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_pso = run_pso_for_catboost_hyperparams(X_train_scaled, y_train_flat)\n",
    "model_cat_pso = CatBoostClassifier(**best_params_pso, class_weights=class_weights_dict, random_seed=42)\n",
    "model_cat_pso.fit(X_train_scaled, y_train_flat)\n",
    "y_pred_cat_pso = model_cat_pso.predict(X_test_scaled)\n",
    "print(\"PSO-tuned CatBoost Accuracy:\", accuracy_score(y_test_flat, y_pred_cat_pso))\n",
    "\n",
    "best_params_aco = run_aco_for_catboost_hyperparams(X_train_scaled, y_train_flat)\n",
    "model_cat_aco = CatBoostClassifier(**best_params_aco, class_weights=class_weights_dict, random_seed=42)\n",
    "model_cat_aco.fit(X_train_scaled, y_train_flat)\n",
    "y_pred_cat_aco = model_cat_aco.predict(X_test_scaled)\n",
    "print(\"ACO-tuned CatBoost Accuracy:\", accuracy_score(y_test_flat, y_pred_cat_aco))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
